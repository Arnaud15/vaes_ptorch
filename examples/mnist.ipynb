{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b40a331-59dc-40fe-99a1-65a5fb5c3f99",
   "metadata": {},
   "source": [
    "## VAEs on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7321b105-99ae-49b5-9b2e-c10ea7056d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as tdata\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from vaes_ptorch import (\n",
    "    CNN,\n",
    "    DeCNN,\n",
    "    GaussianModel,\n",
    "    GaussianVAE,\n",
    "    TrainArgs,\n",
    "    get_mlp,\n",
    "    train,\n",
    ")\n",
    "from vaes_ptorch.args import DivAnnealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dfa96c9-717a-4fc5-8287-7a21962121ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnaud15/miniconda3/envs/ptorch/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1631630778054/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "dataset = torchvision.datasets.MNIST(\n",
    "    root=os.path.expanduser(\"~/vaes_ptorch/data\"),\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=T.ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdac4160-0941-462e-8aa5-762c78315524",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "num_epochs = 2\n",
    "\n",
    "print_every = 1\n",
    "\n",
    "# info_vae = True\n",
    "info_vae = False\n",
    "# start_scale = 0.005\n",
    "# end_scale = 0.005\n",
    "start_scale = 1.0\n",
    "end_scale = 1.0\n",
    "start_epochs = 0\n",
    "linear_epochs = 0\n",
    "\n",
    "in_channels = 1\n",
    "kernel_sizes = [5, 5]\n",
    "out_channels = [16, 32]\n",
    "rev_out_channels = [16, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0dd8b73-b9b7-42d7-b8ca-657dd8a95eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = tdata.DataLoader(\n",
    "    dataset=dataset, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c091cc-0401-4cc4-9676-c745441ec63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = GaussianModel(\n",
    "    model=CNN(\n",
    "        in_channels=1,\n",
    "        out_channels=out_channels,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        bn=True,\n",
    "        f_map_size=7,\n",
    "        out_dim=latent_dim * 2,\n",
    "    ),\n",
    "    out_dim=latent_dim,\n",
    "    min_var=1e-2,\n",
    ")\n",
    "decoder = GaussianModel(\n",
    "    model=DeCNN(\n",
    "        in_dim=latent_dim,\n",
    "        f_map_size=7,\n",
    "        channel_size=32,\n",
    "        out_channels=rev_out_channels,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        bn=True,\n",
    "    ),\n",
    "    out_dim=1,\n",
    "    min_var=0.0,\n",
    "    split_dim=1,\n",
    ")\n",
    "vae = GaussianVAE(encoder=encoder, decoder=decoder)\n",
    "optimizer = torch.optim.Adam(params=vae.parameters(), lr=lr)\n",
    "train_args = TrainArgs(\n",
    "    info_vae=info_vae,\n",
    "    num_epochs=num_epochs,\n",
    "    div_annealing=DivAnnealing(\n",
    "        start_epochs=start_epochs,\n",
    "        linear_epochs=linear_epochs,\n",
    "        start_scale=start_scale,\n",
    "        end_scale=end_scale,\n",
    "    ),\n",
    "    print_every=print_every,\n",
    "    smoothing=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37d8dc2a-76f2-46d4-a980-4aa14791cbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 | Loss: 50.14255 | Div scale: 1.000\n",
      "NLL: 0.15880 | KL: 49.98375\n",
      "Step: 1 | Loss: 65.20184 | Div scale: 1.000\n",
      "NLL: 0.15404 | KL: 200.58148\n",
      "Step: 2 | Loss: 63.74879 | Div scale: 1.000\n",
      "NLL: 0.15173 | KL: 50.51955\n",
      "Step: 3 | Loss: 60.43594 | Div scale: 1.000\n",
      "NLL: 0.14350 | KL: 30.47680\n",
      "Step: 4 | Loss: 61.96195 | Div scale: 1.000\n",
      "NLL: 0.14011 | KL: 75.55598\n",
      "Step: 5 | Loss: 62.88298 | Div scale: 1.000\n",
      "NLL: 0.13578 | KL: 71.03645\n",
      "Step: 6 | Loss: 59.55508 | Div scale: 1.000\n",
      "NLL: 0.13870 | KL: 29.46522\n",
      "Step: 7 | Loss: 54.84325 | Div scale: 1.000\n",
      "NLL: 0.13236 | KL: 12.30449\n",
      "Step: 8 | Loss: 51.66627 | Div scale: 1.000\n",
      "NLL: 0.12932 | KL: 22.94410\n",
      "Step: 9 | Loss: 50.86641 | Div scale: 1.000\n",
      "NLL: 0.12807 | KL: 43.53960\n",
      "Step: 10 | Loss: 49.42676 | Div scale: 1.000\n",
      "NLL: 0.12549 | KL: 36.34443\n",
      "Step: 11 | Loss: 46.67629 | Div scale: 1.000\n",
      "NLL: 0.12682 | KL: 21.79520\n",
      "Step: 12 | Loss: 42.82414 | Div scale: 1.000\n",
      "NLL: 0.12359 | KL: 8.03126\n",
      "Step: 13 | Loss: 39.51317 | Div scale: 1.000\n",
      "NLL: 0.12114 | KL: 9.59328\n",
      "Step: 14 | Loss: 37.19890 | Div scale: 1.000\n",
      "NLL: 0.11978 | KL: 16.25068\n",
      "Step: 15 | Loss: 35.56209 | Div scale: 1.000\n",
      "NLL: 0.11702 | KL: 20.71378\n",
      "Step: 16 | Loss: 34.05879 | Div scale: 1.000\n",
      "NLL: 0.11744 | KL: 20.41168\n",
      "Step: 17 | Loss: 31.79980 | Div scale: 1.000\n",
      "NLL: 0.11264 | KL: 11.35625\n",
      "Step: 18 | Loss: 29.05522 | Div scale: 1.000\n",
      "NLL: 0.11240 | KL: 4.24158\n",
      "Step: 19 | Loss: 26.70869 | Div scale: 1.000\n",
      "NLL: 0.11186 | KL: 5.47811\n",
      "Step: 20 | Loss: 25.12555 | Div scale: 1.000\n",
      "NLL: 0.10933 | KL: 10.76790\n",
      "Step: 21 | Loss: 23.88184 | Div scale: 1.000\n",
      "NLL: 0.10774 | KL: 12.58069\n",
      "Step: 22 | Loss: 22.45174 | Div scale: 1.000\n",
      "NLL: 0.10697 | KL: 9.47394\n",
      "Step: 23 | Loss: 20.94549 | Div scale: 1.000\n",
      "NLL: 0.10914 | KL: 7.28006\n",
      "Step: 24 | Loss: 19.25225 | Div scale: 1.000\n",
      "NLL: 0.10368 | KL: 3.90946\n",
      "Step: 25 | Loss: 17.67744 | Div scale: 1.000\n",
      "NLL: 0.10549 | KL: 3.39861\n",
      "Step: 26 | Loss: 16.56425 | Div scale: 1.000\n",
      "NLL: 0.10262 | KL: 6.44291\n",
      "Step: 27 | Loss: 15.69091 | Div scale: 1.000\n",
      "NLL: 0.10377 | KL: 7.72713\n",
      "Step: 28 | Loss: 14.75223 | Div scale: 1.000\n",
      "NLL: 0.10409 | KL: 6.19997\n",
      "Step: 29 | Loss: 13.77841 | Div scale: 1.000\n",
      "NLL: 0.10258 | KL: 4.91144\n",
      "Step: 30 | Loss: 12.75244 | Div scale: 1.000\n",
      "NLL: 0.09722 | KL: 3.42153\n",
      "Step: 31 | Loss: 11.79836 | Div scale: 1.000\n",
      "NLL: 0.09939 | KL: 3.11227\n",
      "Step: 32 | Loss: 11.07373 | Div scale: 1.000\n",
      "NLL: 0.10031 | KL: 4.45170\n",
      "Step: 33 | Loss: 10.54972 | Div scale: 1.000\n",
      "NLL: 0.09634 | KL: 5.73734\n",
      "Step: 34 | Loss: 10.00818 | Div scale: 1.000\n",
      "NLL: 0.09747 | KL: 5.03684\n",
      "Step: 35 | Loss: 9.39365 | Div scale: 1.000\n",
      "NLL: 0.09649 | KL: 3.76638\n",
      "Step: 36 | Loss: 8.71601 | Div scale: 1.000\n",
      "NLL: 0.09535 | KL: 2.52193\n",
      "Step: 37 | Loss: 8.13067 | Div scale: 1.000\n",
      "NLL: 0.09069 | KL: 2.77184\n",
      "Step: 38 | Loss: 7.72071 | Div scale: 1.000\n",
      "NLL: 0.09205 | KL: 3.93902\n",
      "Step: 39 | Loss: 7.35659 | Div scale: 1.000\n",
      "NLL: 0.09346 | KL: 3.98604\n",
      "Step: 40 | Loss: 6.96460 | Div scale: 1.000\n",
      "NLL: 0.08804 | KL: 3.34874\n",
      "Step: 41 | Loss: 6.50491 | Div scale: 1.000\n",
      "NLL: 0.09049 | KL: 2.27715\n",
      "Step: 42 | Loss: 6.10689 | Div scale: 1.000\n",
      "NLL: 0.08847 | KL: 2.43625\n",
      "Step: 43 | Loss: 5.79515 | Div scale: 1.000\n",
      "NLL: 0.08858 | KL: 2.90093\n",
      "Step: 44 | Loss: 5.52833 | Div scale: 1.000\n",
      "NLL: 0.08736 | KL: 3.03961\n",
      "Step: 45 | Loss: 5.26800 | Div scale: 1.000\n",
      "NLL: 0.08649 | KL: 2.83851\n",
      "Step: 46 | Loss: 4.96418 | Div scale: 1.000\n",
      "NLL: 0.08804 | KL: 2.14180\n",
      "Step: 47 | Loss: 4.69970 | Div scale: 1.000\n",
      "NLL: 0.08638 | KL: 2.23291\n",
      "Step: 48 | Loss: 4.43894 | Div scale: 1.000\n",
      "NLL: 0.08565 | KL: 2.00645\n",
      "Step: 49 | Loss: 4.25609 | Div scale: 1.000\n",
      "NLL: 0.08578 | KL: 2.52472\n",
      "Step: 50 | Loss: 4.10722 | Div scale: 1.000\n",
      "NLL: 0.08474 | KL: 2.68259\n",
      "Step: 51 | Loss: 3.91031 | Div scale: 1.000\n",
      "NLL: 0.08151 | KL: 2.05661\n",
      "Step: 52 | Loss: 3.70368 | Div scale: 1.000\n",
      "NLL: 0.08093 | KL: 1.76311\n",
      "Step: 53 | Loss: 3.52813 | Div scale: 1.000\n",
      "NLL: 0.08335 | KL: 1.86484\n",
      "Step: 54 | Loss: 3.37337 | Div scale: 1.000\n",
      "NLL: 0.08250 | KL: 1.89802\n",
      "Step: 55 | Loss: 3.24304 | Div scale: 1.000\n",
      "NLL: 0.08151 | KL: 1.98855\n",
      "Step: 56 | Loss: 3.10249 | Div scale: 1.000\n",
      "NLL: 0.08178 | KL: 1.75576\n",
      "Step: 57 | Loss: 2.96395 | Div scale: 1.000\n",
      "NLL: 0.07914 | KL: 1.63797\n",
      "Step: 58 | Loss: 2.84843 | Div scale: 1.000\n",
      "NLL: 0.07972 | KL: 1.72901\n",
      "Step: 59 | Loss: 2.77697 | Div scale: 1.000\n",
      "NLL: 0.07787 | KL: 2.05595\n",
      "Step: 60 | Loss: 2.71342 | Div scale: 1.000\n",
      "NLL: 0.07721 | KL: 2.06429\n",
      "Step: 61 | Loss: 2.63576 | Div scale: 1.000\n",
      "NLL: 0.07441 | KL: 1.86240\n",
      "Step: 62 | Loss: 2.55496 | Div scale: 1.000\n",
      "NLL: 0.07640 | KL: 1.75133\n",
      "Step: 63 | Loss: 2.45363 | Div scale: 1.000\n",
      "NLL: 0.07643 | KL: 1.46525\n",
      "Step: 64 | Loss: 2.40731 | Div scale: 1.000\n",
      "NLL: 0.07424 | KL: 1.91621\n",
      "Step: 65 | Loss: 2.35298 | Div scale: 1.000\n",
      "NLL: 0.07318 | KL: 1.79079\n",
      "Step: 66 | Loss: 2.29202 | Div scale: 1.000\n",
      "NLL: 0.07237 | KL: 1.67108\n",
      "Step: 67 | Loss: 2.25988 | Div scale: 1.000\n",
      "NLL: 0.07069 | KL: 1.89990\n",
      "Step: 68 | Loss: 2.18927 | Div scale: 1.000\n",
      "NLL: 0.07056 | KL: 1.48322\n",
      "Step: 69 | Loss: 2.14694 | Div scale: 1.000\n",
      "NLL: 0.06908 | KL: 1.69691\n",
      "Step: 70 | Loss: 2.08247 | Div scale: 1.000\n",
      "NLL: 0.07079 | KL: 1.43146\n",
      "Step: 71 | Loss: 2.03582 | Div scale: 1.000\n",
      "NLL: 0.06847 | KL: 1.54744\n",
      "Step: 72 | Loss: 1.97177 | Div scale: 1.000\n",
      "NLL: 0.07006 | KL: 1.32527\n",
      "Step: 73 | Loss: 1.92154 | Div scale: 1.000\n",
      "NLL: 0.07069 | KL: 1.39875\n",
      "Step: 74 | Loss: 1.88233 | Div scale: 1.000\n",
      "NLL: 0.07036 | KL: 1.45915\n",
      "Step: 75 | Loss: 1.84074 | Div scale: 1.000\n",
      "NLL: 0.06694 | KL: 1.39951\n",
      "Step: 76 | Loss: 1.78203 | Div scale: 1.000\n",
      "NLL: 0.06747 | KL: 1.18611\n",
      "Step: 77 | Loss: 1.73030 | Div scale: 1.000\n",
      "NLL: 0.06671 | KL: 1.19801\n",
      "Step: 78 | Loss: 1.67689 | Div scale: 1.000\n",
      "NLL: 0.06582 | KL: 1.13046\n",
      "Step: 79 | Loss: 1.64973 | Div scale: 1.000\n",
      "NLL: 0.06616 | KL: 1.33906\n",
      "Step: 80 | Loss: 1.63184 | Div scale: 1.000\n",
      "NLL: 0.06752 | KL: 1.40337\n",
      "Step: 81 | Loss: 1.58881 | Div scale: 1.000\n",
      "NLL: 0.06522 | KL: 1.13632\n",
      "Step: 82 | Loss: 1.56349 | Div scale: 1.000\n",
      "NLL: 0.06485 | KL: 1.27078\n",
      "Step: 83 | Loss: 1.54205 | Div scale: 1.000\n",
      "NLL: 0.06455 | KL: 1.28453\n",
      "Step: 84 | Loss: 1.52321 | Div scale: 1.000\n",
      "NLL: 0.06759 | KL: 1.28604\n",
      "Step: 85 | Loss: 1.48019 | Div scale: 1.000\n",
      "NLL: 0.06450 | KL: 1.02845\n",
      "Step: 86 | Loss: 1.45167 | Div scale: 1.000\n",
      "NLL: 0.06438 | KL: 1.13060\n",
      "Step: 87 | Loss: 1.44440 | Div scale: 1.000\n",
      "NLL: 0.06214 | KL: 1.31690\n",
      "Step: 88 | Loss: 1.42464 | Div scale: 1.000\n",
      "NLL: 0.06228 | KL: 1.18447\n",
      "Step: 89 | Loss: 1.39920 | Div scale: 1.000\n",
      "NLL: 0.06363 | KL: 1.10665\n",
      "Step: 90 | Loss: 1.37311 | Div scale: 1.000\n",
      "NLL: 0.06202 | KL: 1.07626\n",
      "Step: 91 | Loss: 1.36548 | Div scale: 1.000\n",
      "NLL: 0.05787 | KL: 1.23895\n",
      "Step: 92 | Loss: 1.34417 | Div scale: 1.000\n",
      "NLL: 0.06319 | KL: 1.08919\n",
      "Step: 93 | Loss: 1.32980 | Div scale: 1.000\n",
      "NLL: 0.06037 | KL: 1.14009\n",
      "Step: 94 | Loss: 1.30503 | Div scale: 1.000\n",
      "NLL: 0.05973 | KL: 1.02234\n",
      "Step: 95 | Loss: 1.28870 | Div scale: 1.000\n",
      "NLL: 0.05962 | KL: 1.08211\n",
      "Step: 96 | Loss: 1.28406 | Div scale: 1.000\n",
      "NLL: 0.05993 | KL: 1.18240\n",
      "Step: 97 | Loss: 1.28765 | Div scale: 1.000\n",
      "NLL: 0.05981 | KL: 1.26020\n",
      "Step: 98 | Loss: 1.26804 | Div scale: 1.000\n",
      "NLL: 0.05972 | KL: 1.03181\n",
      "Step: 99 | Loss: 1.26295 | Div scale: 1.000\n",
      "NLL: 0.05910 | KL: 1.15800\n",
      "Step: 100 | Loss: 1.24487 | Div scale: 1.000\n",
      "NLL: 0.05770 | KL: 1.02443\n",
      "Step: 101 | Loss: 1.25304 | Div scale: 1.000\n",
      "NLL: 0.05667 | KL: 1.26993\n",
      "Step: 102 | Loss: 1.23574 | Div scale: 1.000\n",
      "NLL: 0.05538 | KL: 1.02464\n",
      "Step: 103 | Loss: 1.21529 | Div scale: 1.000\n",
      "NLL: 0.05745 | KL: 0.97383\n",
      "Step: 104 | Loss: 1.22477 | Div scale: 1.000\n",
      "NLL: 0.05565 | KL: 1.25444\n",
      "Step: 105 | Loss: 1.22752 | Div scale: 1.000\n",
      "NLL: 0.05680 | KL: 1.19548\n",
      "Step: 106 | Loss: 1.21129 | Div scale: 1.000\n",
      "NLL: 0.05549 | KL: 1.00969\n",
      "Step: 107 | Loss: 1.20207 | Div scale: 1.000\n",
      "NLL: 0.05426 | KL: 1.06483\n",
      "Step: 108 | Loss: 1.16691 | Div scale: 1.000\n",
      "NLL: 0.05484 | KL: 0.79561\n",
      "Step: 109 | Loss: 1.15715 | Div scale: 1.000\n",
      "NLL: 0.05292 | KL: 1.01637\n",
      "Step: 110 | Loss: 1.14599 | Div scale: 1.000\n",
      "NLL: 0.05610 | KL: 0.98950\n",
      "Step: 111 | Loss: 1.14800 | Div scale: 1.000\n",
      "NLL: 0.05620 | KL: 1.10984\n",
      "Step: 112 | Loss: 1.13577 | Div scale: 1.000\n",
      "NLL: 0.05559 | KL: 0.97014\n",
      "Step: 113 | Loss: 1.11588 | Div scale: 1.000\n",
      "NLL: 0.05350 | KL: 0.88334\n",
      "Step: 114 | Loss: 1.10511 | Div scale: 1.000\n",
      "NLL: 0.05321 | KL: 0.95494\n",
      "Step: 115 | Loss: 1.08876 | Div scale: 1.000\n",
      "NLL: 0.05247 | KL: 0.88920\n",
      "Step: 116 | Loss: 1.07090 | Div scale: 1.000\n",
      "NLL: 0.05202 | KL: 0.85809\n",
      "Step: 117 | Loss: 1.07843 | Div scale: 1.000\n",
      "NLL: 0.05429 | KL: 1.09191\n",
      "Step: 118 | Loss: 1.06754 | Div scale: 1.000\n",
      "NLL: 0.05240 | KL: 0.91715\n",
      "Step: 119 | Loss: 1.05779 | Div scale: 1.000\n",
      "NLL: 0.05148 | KL: 0.91855\n",
      "Step: 120 | Loss: 1.06041 | Div scale: 1.000\n",
      "NLL: 0.05249 | KL: 1.03149\n",
      "Step: 121 | Loss: 1.04310 | Div scale: 1.000\n",
      "NLL: 0.05169 | KL: 0.83561\n",
      "Step: 122 | Loss: 1.03439 | Div scale: 1.000\n",
      "NLL: 0.05190 | KL: 0.90410\n",
      "Step: 123 | Loss: 1.02215 | Div scale: 1.000\n",
      "NLL: 0.05079 | KL: 0.86124\n",
      "Step: 124 | Loss: 1.00492 | Div scale: 1.000\n",
      "NLL: 0.05025 | KL: 0.79962\n",
      "Step: 125 | Loss: 0.99900 | Div scale: 1.000\n",
      "NLL: 0.04859 | KL: 0.89706\n",
      "Step: 126 | Loss: 0.97805 | Div scale: 1.000\n",
      "NLL: 0.05046 | KL: 0.73906\n",
      "Step: 127 | Loss: 0.96217 | Div scale: 1.000\n",
      "NLL: 0.05128 | KL: 0.76802\n",
      "Step: 128 | Loss: 0.95143 | Div scale: 1.000\n",
      "NLL: 0.05014 | KL: 0.80462\n",
      "Step: 129 | Loss: 0.95073 | Div scale: 1.000\n",
      "NLL: 0.05117 | KL: 0.89322\n",
      "Step: 130 | Loss: 0.96236 | Div scale: 1.000\n",
      "NLL: 0.05051 | KL: 1.01657\n",
      "Step: 131 | Loss: 0.96385 | Div scale: 1.000\n",
      "NLL: 0.04887 | KL: 0.92839\n",
      "Step: 132 | Loss: 0.97609 | Div scale: 1.000\n",
      "NLL: 0.04954 | KL: 1.03666\n",
      "Step: 133 | Loss: 0.97570 | Div scale: 1.000\n",
      "NLL: 0.05116 | KL: 0.92103\n",
      "Step: 134 | Loss: 0.96769 | Div scale: 1.000\n",
      "NLL: 0.04934 | KL: 0.84628\n",
      "Step: 135 | Loss: 0.96812 | Div scale: 1.000\n",
      "NLL: 0.04973 | KL: 0.92224\n",
      "Step: 136 | Loss: 0.95844 | Div scale: 1.000\n",
      "NLL: 0.04934 | KL: 0.82204\n",
      "Step: 137 | Loss: 0.95131 | Div scale: 1.000\n",
      "NLL: 0.04829 | KL: 0.83879\n",
      "Step: 138 | Loss: 0.96218 | Div scale: 1.000\n",
      "NLL: 0.04915 | KL: 1.01095\n",
      "Step: 139 | Loss: 0.97240 | Div scale: 1.000\n",
      "NLL: 0.05068 | KL: 1.01371\n",
      "Step: 140 | Loss: 0.95785 | Div scale: 1.000\n",
      "NLL: 0.04760 | KL: 0.77929\n",
      "Step: 141 | Loss: 0.94691 | Div scale: 1.000\n",
      "NLL: 0.04941 | KL: 0.79902\n",
      "Step: 142 | Loss: 0.95729 | Div scale: 1.000\n",
      "NLL: 0.04718 | KL: 1.00353\n",
      "Step: 143 | Loss: 0.93214 | Div scale: 1.000\n",
      "NLL: 0.05029 | KL: 0.65549\n",
      "Step: 144 | Loss: 0.92469 | Div scale: 1.000\n",
      "NLL: 0.04657 | KL: 0.81104\n",
      "Step: 145 | Loss: 0.92002 | Div scale: 1.000\n",
      "NLL: 0.05241 | KL: 0.82562\n",
      "Step: 146 | Loss: 0.91703 | Div scale: 1.000\n",
      "NLL: 0.04783 | KL: 0.84229\n",
      "Step: 147 | Loss: 0.91876 | Div scale: 1.000\n",
      "NLL: 0.04965 | KL: 0.88469\n",
      "Step: 148 | Loss: 0.92047 | Div scale: 1.000\n",
      "NLL: 0.04867 | KL: 0.88714\n",
      "Step: 149 | Loss: 0.90799 | Div scale: 1.000\n",
      "NLL: 0.04823 | KL: 0.74749\n",
      "Step: 150 | Loss: 0.91954 | Div scale: 1.000\n",
      "NLL: 0.04653 | KL: 0.97697\n",
      "Step: 151 | Loss: 0.90041 | Div scale: 1.000\n",
      "NLL: 0.04747 | KL: 0.68074\n",
      "Step: 152 | Loss: 0.89221 | Div scale: 1.000\n",
      "NLL: 0.04709 | KL: 0.77135\n",
      "Step: 153 | Loss: 0.88792 | Div scale: 1.000\n",
      "NLL: 0.04792 | KL: 0.80138\n",
      "Step: 154 | Loss: 0.86664 | Div scale: 1.000\n",
      "NLL: 0.04804 | KL: 0.62702\n",
      "Step: 155 | Loss: 0.85869 | Div scale: 1.000\n",
      "NLL: 0.04785 | KL: 0.73929\n",
      "Step: 156 | Loss: 0.86152 | Div scale: 1.000\n",
      "NLL: 0.04794 | KL: 0.83906\n",
      "Step: 157 | Loss: 0.83414 | Div scale: 1.000\n",
      "NLL: 0.04524 | KL: 0.54251\n",
      "Step: 158 | Loss: 0.83243 | Div scale: 1.000\n",
      "NLL: 0.04742 | KL: 0.76960\n",
      "Step: 159 | Loss: 0.82202 | Div scale: 1.000\n",
      "NLL: 0.04586 | KL: 0.68246\n",
      "Step: 160 | Loss: 0.80985 | Div scale: 1.000\n",
      "NLL: 0.04654 | KL: 0.65377\n",
      "Step: 161 | Loss: 0.80863 | Div scale: 1.000\n",
      "NLL: 0.04734 | KL: 0.75037\n",
      "Step: 162 | Loss: 0.80351 | Div scale: 1.000\n",
      "NLL: 0.04646 | KL: 0.71095\n",
      "Step: 163 | Loss: 0.80512 | Div scale: 1.000\n",
      "NLL: 0.04520 | KL: 0.77441\n",
      "Step: 164 | Loss: 0.81607 | Div scale: 1.000\n",
      "NLL: 0.04596 | KL: 0.86864\n",
      "Step: 165 | Loss: 0.80627 | Div scale: 1.000\n",
      "NLL: 0.04732 | KL: 0.67071\n",
      "Step: 166 | Loss: 0.79797 | Div scale: 1.000\n",
      "NLL: 0.04660 | KL: 0.67672\n",
      "Step: 167 | Loss: 0.80305 | Div scale: 1.000\n",
      "NLL: 0.04536 | KL: 0.80336\n",
      "Step: 168 | Loss: 0.78339 | Div scale: 1.000\n",
      "NLL: 0.04591 | KL: 0.56059\n",
      "Step: 169 | Loss: 0.77242 | Div scale: 1.000\n",
      "NLL: 0.04487 | KL: 0.62880\n",
      "Step: 170 | Loss: 0.76320 | Div scale: 1.000\n",
      "NLL: 0.04567 | KL: 0.63458\n",
      "Step: 171 | Loss: 0.76468 | Div scale: 1.000\n",
      "NLL: 0.04583 | KL: 0.73212\n",
      "Step: 172 | Loss: 0.77292 | Div scale: 1.000\n",
      "NLL: 0.04612 | KL: 0.80098\n",
      "Step: 173 | Loss: 0.77196 | Div scale: 1.000\n",
      "NLL: 0.04493 | KL: 0.71843\n",
      "Step: 174 | Loss: 0.76002 | Div scale: 1.000\n",
      "NLL: 0.04603 | KL: 0.60655\n",
      "Step: 175 | Loss: 0.76704 | Div scale: 1.000\n",
      "NLL: 0.04544 | KL: 0.78479\n",
      "Step: 176 | Loss: 0.76246 | Div scale: 1.000\n",
      "NLL: 0.04412 | KL: 0.67704\n",
      "Step: 177 | Loss: 0.77639 | Div scale: 1.000\n",
      "NLL: 0.04655 | KL: 0.85525\n",
      "Step: 178 | Loss: 0.77477 | Div scale: 1.000\n",
      "NLL: 0.04587 | KL: 0.71437\n",
      "Step: 179 | Loss: 0.77008 | Div scale: 1.000\n",
      "NLL: 0.04420 | KL: 0.68361\n",
      "Step: 180 | Loss: 0.77176 | Div scale: 1.000\n",
      "NLL: 0.04545 | KL: 0.74142\n",
      "Step: 181 | Loss: 0.77205 | Div scale: 1.000\n",
      "NLL: 0.04423 | KL: 0.73046\n",
      "Step: 182 | Loss: 0.76185 | Div scale: 1.000\n",
      "NLL: 0.04405 | KL: 0.62603\n",
      "Step: 183 | Loss: 0.77135 | Div scale: 1.000\n",
      "NLL: 0.04421 | KL: 0.81265\n",
      "Step: 184 | Loss: 0.77047 | Div scale: 1.000\n",
      "NLL: 0.04581 | KL: 0.71677\n",
      "Step: 185 | Loss: 0.76124 | Div scale: 1.000\n",
      "NLL: 0.04598 | KL: 0.63218\n",
      "Step: 186 | Loss: 0.76198 | Div scale: 1.000\n",
      "NLL: 0.04546 | KL: 0.72315\n",
      "Step: 187 | Loss: 0.74918 | Div scale: 1.000\n",
      "NLL: 0.04442 | KL: 0.58956\n",
      "Step: 188 | Loss: 0.75344 | Div scale: 1.000\n",
      "NLL: 0.04370 | KL: 0.74811\n",
      "Step: 189 | Loss: 0.74644 | Div scale: 1.000\n",
      "NLL: 0.04382 | KL: 0.63956\n",
      "Step: 190 | Loss: 0.73863 | Div scale: 1.000\n",
      "NLL: 0.04361 | KL: 0.62475\n",
      "Step: 191 | Loss: 0.72711 | Div scale: 1.000\n",
      "NLL: 0.04363 | KL: 0.57978\n",
      "Step: 192 | Loss: 0.73988 | Div scale: 1.000\n",
      "NLL: 0.04490 | KL: 0.80995\n",
      "Step: 193 | Loss: 0.73015 | Div scale: 1.000\n",
      "NLL: 0.04415 | KL: 0.59841\n",
      "Step: 194 | Loss: 0.72279 | Div scale: 1.000\n",
      "NLL: 0.04497 | KL: 0.61160\n",
      "Step: 195 | Loss: 0.71629 | Div scale: 1.000\n",
      "NLL: 0.04458 | KL: 0.61324\n",
      "Step: 196 | Loss: 0.71760 | Div scale: 1.000\n",
      "NLL: 0.04448 | KL: 0.68487\n",
      "Step: 197 | Loss: 0.70965 | Div scale: 1.000\n",
      "NLL: 0.04435 | KL: 0.59375\n",
      "Step: 198 | Loss: 0.70938 | Div scale: 1.000\n",
      "NLL: 0.04482 | KL: 0.66218\n",
      "Step: 199 | Loss: 0.71950 | Div scale: 1.000\n",
      "NLL: 0.04408 | KL: 0.76646\n",
      "Step: 200 | Loss: 0.70944 | Div scale: 1.000\n",
      "NLL: 0.04506 | KL: 0.57380\n",
      "Step: 201 | Loss: 0.70596 | Div scale: 1.000\n",
      "NLL: 0.04471 | KL: 0.62996\n",
      "Step: 202 | Loss: 0.69665 | Div scale: 1.000\n",
      "NLL: 0.04187 | KL: 0.57101\n",
      "Step: 203 | Loss: 0.69862 | Div scale: 1.000\n",
      "NLL: 0.04404 | KL: 0.67230\n",
      "Step: 204 | Loss: 0.69787 | Div scale: 1.000\n",
      "NLL: 0.04346 | KL: 0.64770\n",
      "Step: 205 | Loss: 0.70223 | Div scale: 1.000\n",
      "NLL: 0.04258 | KL: 0.69885\n",
      "Step: 206 | Loss: 0.69945 | Div scale: 1.000\n",
      "NLL: 0.04204 | KL: 0.63241\n",
      "Step: 207 | Loss: 0.69503 | Div scale: 1.000\n",
      "NLL: 0.04300 | KL: 0.61221\n",
      "Step: 208 | Loss: 0.68843 | Div scale: 1.000\n",
      "NLL: 0.04362 | KL: 0.58540\n",
      "Step: 209 | Loss: 0.68482 | Div scale: 1.000\n",
      "NLL: 0.04390 | KL: 0.60847\n",
      "Step: 210 | Loss: 0.67996 | Div scale: 1.000\n",
      "NLL: 0.04364 | KL: 0.59258\n",
      "Step: 211 | Loss: 0.67314 | Div scale: 1.000\n",
      "NLL: 0.04436 | KL: 0.56742\n",
      "Step: 212 | Loss: 0.66060 | Div scale: 1.000\n",
      "NLL: 0.04267 | KL: 0.50504\n",
      "Step: 213 | Loss: 0.64762 | Div scale: 1.000\n",
      "NLL: 0.04236 | KL: 0.48842\n",
      "Step: 214 | Loss: 0.64065 | Div scale: 1.000\n",
      "NLL: 0.04166 | KL: 0.53624\n",
      "Step: 215 | Loss: 0.63482 | Div scale: 1.000\n",
      "NLL: 0.04195 | KL: 0.54042\n",
      "Step: 216 | Loss: 0.63216 | Div scale: 1.000\n",
      "NLL: 0.04244 | KL: 0.56584\n",
      "Step: 217 | Loss: 0.62481 | Div scale: 1.000\n",
      "NLL: 0.04335 | KL: 0.51532\n",
      "Step: 218 | Loss: 0.63361 | Div scale: 1.000\n",
      "NLL: 0.04353 | KL: 0.66922\n",
      "Step: 219 | Loss: 0.62923 | Div scale: 1.000\n",
      "NLL: 0.04218 | KL: 0.54764\n",
      "Step: 220 | Loss: 0.61443 | Div scale: 1.000\n",
      "NLL: 0.04099 | KL: 0.44027\n",
      "Step: 221 | Loss: 0.61321 | Div scale: 1.000\n",
      "NLL: 0.04242 | KL: 0.55984\n",
      "Step: 222 | Loss: 0.61480 | Div scale: 1.000\n",
      "NLL: 0.04123 | KL: 0.58786\n",
      "Step: 223 | Loss: 0.60800 | Div scale: 1.000\n",
      "NLL: 0.04210 | KL: 0.50470\n",
      "Step: 224 | Loss: 0.60374 | Div scale: 1.000\n",
      "NLL: 0.04198 | KL: 0.52337\n",
      "Step: 225 | Loss: 0.59861 | Div scale: 1.000\n",
      "NLL: 0.04355 | KL: 0.50896\n",
      "Step: 226 | Loss: 0.59933 | Div scale: 1.000\n",
      "NLL: 0.04268 | KL: 0.56309\n",
      "Step: 227 | Loss: 0.59364 | Div scale: 1.000\n",
      "NLL: 0.04154 | KL: 0.50094\n",
      "Step: 228 | Loss: 0.58712 | Div scale: 1.000\n",
      "NLL: 0.04270 | KL: 0.48567\n",
      "Step: 229 | Loss: 0.58765 | Div scale: 1.000\n",
      "NLL: 0.04114 | KL: 0.55134\n",
      "Step: 230 | Loss: 0.59045 | Div scale: 1.000\n",
      "NLL: 0.04343 | KL: 0.57221\n",
      "Step: 231 | Loss: 0.59130 | Div scale: 1.000\n",
      "NLL: 0.04406 | KL: 0.55483\n",
      "Step: 232 | Loss: 0.59325 | Div scale: 1.000\n",
      "NLL: 0.04070 | KL: 0.57014\n",
      "Step: 233 | Loss: 0.60016 | Div scale: 1.000\n",
      "NLL: 0.04163 | KL: 0.62074\n",
      "Step: 234 | Loss: 0.59079 | Div scale: 1.000\n",
      "NLL: 0.04227 | KL: 0.46414\n",
      "Step: 235 | Loss: 0.59545 | Div scale: 1.000\n",
      "NLL: 0.03979 | KL: 0.59765\n",
      "Step: 236 | Loss: 0.59237 | Div scale: 1.000\n",
      "NLL: 0.04261 | KL: 0.52200\n",
      "Step: 237 | Loss: 0.58170 | Div scale: 1.000\n",
      "NLL: 0.04070 | KL: 0.44495\n",
      "Step: 238 | Loss: 0.57568 | Div scale: 1.000\n",
      "NLL: 0.04133 | KL: 0.48026\n",
      "Step: 239 | Loss: 0.57570 | Div scale: 1.000\n",
      "NLL: 0.04248 | KL: 0.53333\n",
      "Step: 240 | Loss: 0.57599 | Div scale: 1.000\n",
      "NLL: 0.04315 | KL: 0.53546\n",
      "Step: 241 | Loss: 0.57219 | Div scale: 1.000\n",
      "NLL: 0.04012 | KL: 0.49789\n",
      "Step: 242 | Loss: 0.56890 | Div scale: 1.000\n",
      "NLL: 0.04028 | KL: 0.49900\n",
      "Step: 243 | Loss: 0.56738 | Div scale: 1.000\n",
      "NLL: 0.04078 | KL: 0.51293\n",
      "Step: 244 | Loss: 0.56575 | Div scale: 1.000\n",
      "NLL: 0.04142 | KL: 0.50968\n",
      "Step: 245 | Loss: 0.56929 | Div scale: 1.000\n",
      "NLL: 0.04000 | KL: 0.56116\n",
      "Step: 246 | Loss: 0.56153 | Div scale: 1.000\n",
      "NLL: 0.03994 | KL: 0.45170\n",
      "Step: 247 | Loss: 0.55538 | Div scale: 1.000\n",
      "NLL: 0.04004 | KL: 0.46005\n",
      "Step: 248 | Loss: 0.55581 | Div scale: 1.000\n",
      "NLL: 0.04150 | KL: 0.51815\n",
      "Step: 249 | Loss: 0.55326 | Div scale: 1.000\n",
      "NLL: 0.04206 | KL: 0.48823\n",
      "Step: 250 | Loss: 0.55121 | Div scale: 1.000\n",
      "NLL: 0.03832 | KL: 0.49443\n",
      "Step: 251 | Loss: 0.54597 | Div scale: 1.000\n",
      "NLL: 0.04019 | KL: 0.45861\n",
      "Step: 252 | Loss: 0.53486 | Div scale: 1.000\n",
      "NLL: 0.03999 | KL: 0.39490\n",
      "Step: 253 | Loss: 0.52267 | Div scale: 1.000\n",
      "NLL: 0.04081 | KL: 0.37215\n",
      "Step: 254 | Loss: 0.51743 | Div scale: 1.000\n",
      "NLL: 0.04040 | KL: 0.42990\n",
      "Step: 255 | Loss: 0.51493 | Div scale: 1.000\n",
      "NLL: 0.03994 | KL: 0.45248\n",
      "Step: 256 | Loss: 0.50981 | Div scale: 1.000\n",
      "NLL: 0.04116 | KL: 0.42252\n",
      "Step: 257 | Loss: 0.51213 | Div scale: 1.000\n",
      "NLL: 0.03958 | KL: 0.49342\n",
      "Step: 258 | Loss: 0.51384 | Div scale: 1.000\n",
      "NLL: 0.04194 | KL: 0.48736\n",
      "Step: 259 | Loss: 0.50611 | Div scale: 1.000\n",
      "NLL: 0.03970 | KL: 0.39685\n",
      "Step: 260 | Loss: 0.50039 | Div scale: 1.000\n",
      "NLL: 0.04053 | KL: 0.40831\n",
      "Step: 261 | Loss: 0.49999 | Div scale: 1.000\n",
      "NLL: 0.04043 | KL: 0.45603\n",
      "Step: 262 | Loss: 0.49736 | Div scale: 1.000\n",
      "NLL: 0.03911 | KL: 0.43455\n",
      "Step: 263 | Loss: 0.49642 | Div scale: 1.000\n",
      "NLL: 0.03988 | KL: 0.44807\n",
      "Step: 264 | Loss: 0.49958 | Div scale: 1.000\n",
      "NLL: 0.04154 | KL: 0.48652\n",
      "Step: 265 | Loss: 0.50493 | Div scale: 1.000\n",
      "NLL: 0.03941 | KL: 0.51361\n",
      "Step: 266 | Loss: 0.49595 | Div scale: 1.000\n",
      "NLL: 0.04012 | KL: 0.37507\n",
      "Step: 267 | Loss: 0.49387 | Div scale: 1.000\n",
      "NLL: 0.04016 | KL: 0.43499\n",
      "Step: 268 | Loss: 0.50647 | Div scale: 1.000\n",
      "NLL: 0.03993 | KL: 0.57990\n",
      "Step: 269 | Loss: 0.49763 | Div scale: 1.000\n",
      "NLL: 0.03858 | KL: 0.37950\n",
      "Step: 270 | Loss: 0.49702 | Div scale: 1.000\n",
      "NLL: 0.04166 | KL: 0.44988\n",
      "Step: 271 | Loss: 0.49340 | Div scale: 1.000\n",
      "NLL: 0.03887 | KL: 0.42198\n",
      "Step: 272 | Loss: 0.49973 | Div scale: 1.000\n",
      "NLL: 0.04032 | KL: 0.51635\n",
      "Step: 273 | Loss: 0.49660 | Div scale: 1.000\n",
      "NLL: 0.03967 | KL: 0.42872\n",
      "Step: 274 | Loss: 0.49459 | Div scale: 1.000\n",
      "NLL: 0.03888 | KL: 0.43761\n",
      "Step: 275 | Loss: 0.49023 | Div scale: 1.000\n",
      "NLL: 0.03948 | KL: 0.41156\n",
      "Step: 276 | Loss: 0.48218 | Div scale: 1.000\n",
      "NLL: 0.03912 | KL: 0.37057\n",
      "Step: 277 | Loss: 0.48027 | Div scale: 1.000\n",
      "NLL: 0.03803 | KL: 0.42510\n",
      "Step: 278 | Loss: 0.47772 | Div scale: 1.000\n",
      "NLL: 0.03830 | KL: 0.41642\n",
      "Step: 279 | Loss: 0.48004 | Div scale: 1.000\n",
      "NLL: 0.03995 | KL: 0.46102\n",
      "Step: 280 | Loss: 0.48223 | Div scale: 1.000\n",
      "NLL: 0.03872 | KL: 0.46316\n",
      "Step: 281 | Loss: 0.47634 | Div scale: 1.000\n",
      "NLL: 0.03770 | KL: 0.38566\n",
      "Step: 282 | Loss: 0.46767 | Div scale: 1.000\n",
      "NLL: 0.03787 | KL: 0.35179\n",
      "Step: 283 | Loss: 0.46913 | Div scale: 1.000\n",
      "NLL: 0.03925 | KL: 0.44299\n",
      "Step: 284 | Loss: 0.46532 | Div scale: 1.000\n",
      "NLL: 0.03778 | KL: 0.39328\n",
      "Step: 285 | Loss: 0.46800 | Div scale: 1.000\n",
      "NLL: 0.03886 | KL: 0.45327\n",
      "Step: 286 | Loss: 0.46797 | Div scale: 1.000\n",
      "NLL: 0.03804 | KL: 0.42964\n",
      "Step: 287 | Loss: 0.46284 | Div scale: 1.000\n",
      "NLL: 0.03883 | KL: 0.37788\n",
      "Step: 288 | Loss: 0.47110 | Div scale: 1.000\n",
      "NLL: 0.03790 | KL: 0.50751\n",
      "Step: 289 | Loss: 0.47951 | Div scale: 1.000\n",
      "NLL: 0.03896 | KL: 0.51628\n",
      "Step: 290 | Loss: 0.48496 | Div scale: 1.000\n",
      "NLL: 0.03939 | KL: 0.49459\n",
      "Step: 291 | Loss: 0.47976 | Div scale: 1.000\n",
      "NLL: 0.03809 | KL: 0.39483\n",
      "Step: 292 | Loss: 0.47633 | Div scale: 1.000\n",
      "NLL: 0.03953 | KL: 0.40596\n",
      "Step: 293 | Loss: 0.47794 | Div scale: 1.000\n",
      "NLL: 0.03912 | KL: 0.45335\n",
      "Step: 294 | Loss: 0.47210 | Div scale: 1.000\n",
      "NLL: 0.03923 | KL: 0.38031\n",
      "Step: 295 | Loss: 0.47042 | Div scale: 1.000\n",
      "NLL: 0.03787 | KL: 0.41734\n",
      "Step: 296 | Loss: 0.46382 | Div scale: 1.000\n",
      "NLL: 0.03782 | KL: 0.36667\n",
      "Step: 297 | Loss: 0.45876 | Div scale: 1.000\n",
      "NLL: 0.03754 | KL: 0.37564\n",
      "Step: 298 | Loss: 0.45594 | Div scale: 1.000\n",
      "NLL: 0.03851 | KL: 0.39206\n",
      "Step: 299 | Loss: 0.45527 | Div scale: 1.000\n",
      "NLL: 0.03710 | KL: 0.41218\n",
      "Step: 300 | Loss: 0.45547 | Div scale: 1.000\n",
      "NLL: 0.03562 | KL: 0.42160\n",
      "Step: 301 | Loss: 0.45109 | Div scale: 1.000\n",
      "NLL: 0.03739 | KL: 0.37431\n",
      "Step: 302 | Loss: 0.44954 | Div scale: 1.000\n",
      "NLL: 0.03732 | KL: 0.39826\n",
      "Step: 303 | Loss: 0.44393 | Div scale: 1.000\n",
      "NLL: 0.03807 | KL: 0.35540\n",
      "Step: 304 | Loss: 0.44606 | Div scale: 1.000\n",
      "NLL: 0.03796 | KL: 0.42727\n",
      "Step: 305 | Loss: 0.44484 | Div scale: 1.000\n",
      "NLL: 0.03805 | KL: 0.39580\n",
      "Step: 306 | Loss: 0.43965 | Div scale: 1.000\n",
      "NLL: 0.03666 | KL: 0.35628\n",
      "Step: 307 | Loss: 0.43525 | Div scale: 1.000\n",
      "NLL: 0.03810 | KL: 0.35756\n",
      "Step: 308 | Loss: 0.44015 | Div scale: 1.000\n",
      "NLL: 0.03703 | KL: 0.44725\n",
      "Step: 309 | Loss: 0.43589 | Div scale: 1.000\n",
      "NLL: 0.03683 | KL: 0.36069\n",
      "Step: 310 | Loss: 0.43958 | Div scale: 1.000\n",
      "NLL: 0.03745 | KL: 0.43535\n",
      "Step: 311 | Loss: 0.43391 | Div scale: 1.000\n",
      "NLL: 0.03847 | KL: 0.34442\n",
      "Step: 312 | Loss: 0.43077 | Div scale: 1.000\n",
      "NLL: 0.03693 | KL: 0.36553\n",
      "Step: 313 | Loss: 0.43200 | Div scale: 1.000\n",
      "NLL: 0.03682 | KL: 0.40625\n",
      "Step: 314 | Loss: 0.43099 | Div scale: 1.000\n",
      "NLL: 0.03655 | KL: 0.38542\n",
      "Step: 315 | Loss: 0.42581 | Div scale: 1.000\n",
      "NLL: 0.03630 | KL: 0.34288\n",
      "Step: 316 | Loss: 0.43142 | Div scale: 1.000\n",
      "NLL: 0.03696 | KL: 0.44495\n",
      "Step: 317 | Loss: 0.43625 | Div scale: 1.000\n",
      "NLL: 0.03701 | KL: 0.44267\n",
      "Step: 318 | Loss: 0.43394 | Div scale: 1.000\n",
      "NLL: 0.03689 | KL: 0.37632\n",
      "Step: 319 | Loss: 0.44067 | Div scale: 1.000\n",
      "NLL: 0.03920 | KL: 0.46198\n",
      "Step: 320 | Loss: 0.44227 | Div scale: 1.000\n",
      "NLL: 0.03711 | KL: 0.41955\n",
      "Step: 321 | Loss: 0.44478 | Div scale: 1.000\n",
      "NLL: 0.03681 | KL: 0.43058\n",
      "Step: 322 | Loss: 0.43878 | Div scale: 1.000\n",
      "NLL: 0.03655 | KL: 0.34818\n",
      "Step: 323 | Loss: 0.43845 | Div scale: 1.000\n",
      "NLL: 0.03617 | KL: 0.39937\n",
      "Step: 324 | Loss: 0.44640 | Div scale: 1.000\n",
      "NLL: 0.03730 | KL: 0.48063\n",
      "Step: 325 | Loss: 0.44463 | Div scale: 1.000\n",
      "NLL: 0.03499 | KL: 0.39367\n",
      "Step: 326 | Loss: 0.44992 | Div scale: 1.000\n",
      "NLL: 0.03627 | KL: 0.46134\n",
      "Step: 327 | Loss: 0.44091 | Div scale: 1.000\n",
      "NLL: 0.03565 | KL: 0.32415\n",
      "Step: 328 | Loss: 0.44120 | Div scale: 1.000\n",
      "NLL: 0.03587 | KL: 0.40790\n",
      "Step: 329 | Loss: 0.43211 | Div scale: 1.000\n",
      "NLL: 0.03613 | KL: 0.31420\n",
      "Step: 330 | Loss: 0.42990 | Div scale: 1.000\n",
      "NLL: 0.03694 | KL: 0.37310\n",
      "Step: 331 | Loss: 0.42588 | Div scale: 1.000\n",
      "NLL: 0.03660 | KL: 0.35306\n",
      "Step: 332 | Loss: 0.42196 | Div scale: 1.000\n",
      "NLL: 0.03411 | KL: 0.35255\n",
      "Step: 333 | Loss: 0.42144 | Div scale: 1.000\n",
      "NLL: 0.03612 | KL: 0.38064\n",
      "Step: 334 | Loss: 0.41802 | Div scale: 1.000\n",
      "NLL: 0.03612 | KL: 0.35115\n",
      "Step: 335 | Loss: 0.42344 | Div scale: 1.000\n",
      "NLL: 0.03638 | KL: 0.43579\n",
      "Step: 336 | Loss: 0.41951 | Div scale: 1.000\n",
      "NLL: 0.03634 | KL: 0.34788\n",
      "Step: 337 | Loss: 0.41798 | Div scale: 1.000\n",
      "NLL: 0.03487 | KL: 0.36934\n",
      "Step: 338 | Loss: 0.41678 | Div scale: 1.000\n",
      "NLL: 0.03620 | KL: 0.36974\n",
      "Step: 339 | Loss: 0.42092 | Div scale: 1.000\n",
      "NLL: 0.03815 | KL: 0.42008\n",
      "Step: 340 | Loss: 0.41914 | Div scale: 1.000\n",
      "NLL: 0.03523 | KL: 0.36780\n",
      "Step: 341 | Loss: 0.42088 | Div scale: 1.000\n",
      "NLL: 0.03661 | KL: 0.40001\n",
      "Step: 342 | Loss: 0.42169 | Div scale: 1.000\n",
      "NLL: 0.03574 | KL: 0.39319\n",
      "Step: 343 | Loss: 0.41856 | Div scale: 1.000\n",
      "NLL: 0.03645 | KL: 0.35393\n",
      "Step: 344 | Loss: 0.42047 | Div scale: 1.000\n",
      "NLL: 0.03467 | KL: 0.40296\n",
      "Step: 345 | Loss: 0.41574 | Div scale: 1.000\n",
      "NLL: 0.03546 | KL: 0.33772\n",
      "Step: 346 | Loss: 0.41289 | Div scale: 1.000\n",
      "NLL: 0.03535 | KL: 0.35196\n",
      "Step: 347 | Loss: 0.40831 | Div scale: 1.000\n",
      "NLL: 0.03502 | KL: 0.33204\n",
      "Step: 348 | Loss: 0.41200 | Div scale: 1.000\n",
      "NLL: 0.03552 | KL: 0.40973\n",
      "Step: 349 | Loss: 0.41571 | Div scale: 1.000\n",
      "NLL: 0.03563 | KL: 0.41348\n",
      "Step: 350 | Loss: 0.40928 | Div scale: 1.000\n",
      "NLL: 0.03552 | KL: 0.31588\n",
      "Step: 351 | Loss: 0.41484 | Div scale: 1.000\n",
      "NLL: 0.03465 | KL: 0.43016\n",
      "Step: 352 | Loss: 0.41519 | Div scale: 1.000\n",
      "NLL: 0.03477 | KL: 0.38363\n",
      "Step: 353 | Loss: 0.40978 | Div scale: 1.000\n",
      "NLL: 0.03608 | KL: 0.32504\n",
      "Step: 354 | Loss: 0.41492 | Div scale: 1.000\n",
      "NLL: 0.03535 | KL: 0.42578\n",
      "Step: 355 | Loss: 0.41553 | Div scale: 1.000\n",
      "NLL: 0.03485 | KL: 0.38620\n",
      "Step: 356 | Loss: 0.40874 | Div scale: 1.000\n",
      "NLL: 0.03691 | KL: 0.31069\n",
      "Step: 357 | Loss: 0.40061 | Div scale: 1.000\n",
      "NLL: 0.03551 | KL: 0.29195\n",
      "Step: 358 | Loss: 0.39785 | Div scale: 1.000\n",
      "NLL: 0.03490 | KL: 0.33814\n",
      "Step: 359 | Loss: 0.40285 | Div scale: 1.000\n",
      "NLL: 0.03554 | KL: 0.41230\n",
      "Step: 360 | Loss: 0.39697 | Div scale: 1.000\n",
      "NLL: 0.03466 | KL: 0.30941\n",
      "Step: 361 | Loss: 0.40438 | Div scale: 1.000\n",
      "NLL: 0.03634 | KL: 0.43465\n",
      "Step: 362 | Loss: 0.39798 | Div scale: 1.000\n",
      "NLL: 0.03516 | KL: 0.30526\n",
      "Step: 363 | Loss: 0.40233 | Div scale: 1.000\n",
      "NLL: 0.03561 | KL: 0.40586\n",
      "Step: 364 | Loss: 0.39572 | Div scale: 1.000\n",
      "NLL: 0.03469 | KL: 0.30157\n",
      "Step: 365 | Loss: 0.39490 | Div scale: 1.000\n",
      "NLL: 0.03619 | KL: 0.35135\n",
      "Step: 366 | Loss: 0.39074 | Div scale: 1.000\n",
      "NLL: 0.03413 | KL: 0.31918\n",
      "Step: 367 | Loss: 0.38632 | Div scale: 1.000\n",
      "NLL: 0.03500 | KL: 0.31147\n",
      "Step: 368 | Loss: 0.38514 | Div scale: 1.000\n",
      "NLL: 0.03572 | KL: 0.33879\n",
      "Step: 369 | Loss: 0.38548 | Div scale: 1.000\n",
      "NLL: 0.03493 | KL: 0.35363\n",
      "Step: 370 | Loss: 0.38297 | Div scale: 1.000\n",
      "NLL: 0.03498 | KL: 0.32543\n",
      "Step: 371 | Loss: 0.37765 | Div scale: 1.000\n",
      "NLL: 0.03550 | KL: 0.29425\n",
      "Step: 372 | Loss: 0.37930 | Div scale: 1.000\n",
      "NLL: 0.03466 | KL: 0.35947\n",
      "Step: 373 | Loss: 0.38108 | Div scale: 1.000\n",
      "NLL: 0.03459 | KL: 0.36253\n",
      "Step: 374 | Loss: 0.38315 | Div scale: 1.000\n",
      "NLL: 0.03507 | KL: 0.36674\n",
      "Step: 375 | Loss: 0.38244 | Div scale: 1.000\n",
      "NLL: 0.03474 | KL: 0.34127\n",
      "Step: 376 | Loss: 0.38047 | Div scale: 1.000\n",
      "NLL: 0.03656 | KL: 0.32620\n",
      "Step: 377 | Loss: 0.37974 | Div scale: 1.000\n",
      "NLL: 0.03356 | KL: 0.33962\n",
      "Step: 378 | Loss: 0.37609 | Div scale: 1.000\n",
      "NLL: 0.03424 | KL: 0.30898\n",
      "Step: 379 | Loss: 0.37213 | Div scale: 1.000\n",
      "NLL: 0.03476 | KL: 0.30174\n",
      "Step: 380 | Loss: 0.36695 | Div scale: 1.000\n",
      "NLL: 0.03671 | KL: 0.28361\n",
      "Step: 381 | Loss: 0.36939 | Div scale: 1.000\n",
      "NLL: 0.03511 | KL: 0.35626\n",
      "Step: 382 | Loss: 0.36224 | Div scale: 1.000\n",
      "NLL: 0.03549 | KL: 0.26240\n",
      "Step: 383 | Loss: 0.36345 | Div scale: 1.000\n",
      "NLL: 0.03477 | KL: 0.33953\n",
      "Step: 384 | Loss: 0.36693 | Div scale: 1.000\n",
      "NLL: 0.03424 | KL: 0.36407\n",
      "Step: 385 | Loss: 0.36619 | Div scale: 1.000\n",
      "NLL: 0.03459 | KL: 0.32488\n",
      "Step: 386 | Loss: 0.36697 | Div scale: 1.000\n",
      "NLL: 0.03445 | KL: 0.33962\n",
      "Step: 387 | Loss: 0.36273 | Div scale: 1.000\n",
      "NLL: 0.03521 | KL: 0.28937\n",
      "Step: 388 | Loss: 0.36222 | Div scale: 1.000\n",
      "NLL: 0.03624 | KL: 0.32133\n",
      "Step: 389 | Loss: 0.36081 | Div scale: 1.000\n",
      "NLL: 0.03412 | KL: 0.31398\n",
      "Step: 390 | Loss: 0.35776 | Div scale: 1.000\n",
      "NLL: 0.03417 | KL: 0.29615\n",
      "Step: 391 | Loss: 0.35543 | Div scale: 1.000\n",
      "NLL: 0.03509 | KL: 0.29937\n",
      "Step: 392 | Loss: 0.35540 | Div scale: 1.000\n",
      "NLL: 0.03405 | KL: 0.32113\n",
      "Step: 393 | Loss: 0.36454 | Div scale: 1.000\n",
      "NLL: 0.03570 | KL: 0.41111\n",
      "Step: 394 | Loss: 0.35792 | Div scale: 1.000\n",
      "NLL: 0.03609 | KL: 0.26222\n",
      "Step: 395 | Loss: 0.35245 | Div scale: 1.000\n",
      "NLL: 0.03398 | KL: 0.26924\n",
      "Step: 396 | Loss: 0.35070 | Div scale: 1.000\n",
      "NLL: 0.03481 | KL: 0.30015\n",
      "Step: 397 | Loss: 0.35040 | Div scale: 1.000\n",
      "NLL: 0.03438 | KL: 0.31328\n",
      "Step: 398 | Loss: 0.35484 | Div scale: 1.000\n",
      "NLL: 0.03450 | KL: 0.36036\n",
      "Step: 399 | Loss: 0.35342 | Div scale: 1.000\n",
      "NLL: 0.03564 | KL: 0.30495\n",
      "Step: 400 | Loss: 0.35262 | Div scale: 1.000\n",
      "NLL: 0.03554 | KL: 0.30986\n",
      "Step: 401 | Loss: 0.34831 | Div scale: 1.000\n",
      "NLL: 0.03377 | KL: 0.27573\n",
      "Step: 402 | Loss: 0.34069 | Div scale: 1.000\n",
      "NLL: 0.03561 | KL: 0.23658\n",
      "Step: 403 | Loss: 0.33953 | Div scale: 1.000\n",
      "NLL: 0.03565 | KL: 0.29340\n",
      "Step: 404 | Loss: 0.34148 | Div scale: 1.000\n",
      "NLL: 0.03403 | KL: 0.32504\n",
      "Step: 405 | Loss: 0.34505 | Div scale: 1.000\n",
      "NLL: 0.03556 | KL: 0.34155\n",
      "Step: 406 | Loss: 0.34519 | Div scale: 1.000\n",
      "NLL: 0.03390 | KL: 0.31255\n",
      "Step: 407 | Loss: 0.34234 | Div scale: 1.000\n",
      "NLL: 0.03383 | KL: 0.28289\n",
      "Step: 408 | Loss: 0.34246 | Div scale: 1.000\n",
      "NLL: 0.03400 | KL: 0.30955\n",
      "Step: 409 | Loss: 0.34080 | Div scale: 1.000\n",
      "NLL: 0.03439 | KL: 0.29148\n",
      "Step: 410 | Loss: 0.33301 | Div scale: 1.000\n",
      "NLL: 0.03603 | KL: 0.22685\n",
      "Step: 411 | Loss: 0.33179 | Div scale: 1.000\n",
      "NLL: 0.03483 | KL: 0.28600\n",
      "Step: 412 | Loss: 0.33143 | Div scale: 1.000\n",
      "NLL: 0.03376 | KL: 0.29439\n",
      "Step: 413 | Loss: 0.33271 | Div scale: 1.000\n",
      "NLL: 0.03405 | KL: 0.31025\n",
      "Step: 414 | Loss: 0.33557 | Div scale: 1.000\n",
      "NLL: 0.03565 | KL: 0.32564\n",
      "Step: 415 | Loss: 0.33838 | Div scale: 1.000\n",
      "NLL: 0.03413 | KL: 0.32952\n",
      "Step: 416 | Loss: 0.33654 | Div scale: 1.000\n",
      "NLL: 0.03449 | KL: 0.28552\n",
      "Step: 417 | Loss: 0.33159 | Div scale: 1.000\n",
      "NLL: 0.03590 | KL: 0.25111\n",
      "Step: 418 | Loss: 0.33033 | Div scale: 1.000\n",
      "NLL: 0.03462 | KL: 0.28441\n",
      "Step: 419 | Loss: 0.32958 | Div scale: 1.000\n",
      "NLL: 0.03377 | KL: 0.28898\n",
      "Step: 420 | Loss: 0.32748 | Div scale: 1.000\n",
      "NLL: 0.03340 | KL: 0.27524\n",
      "Step: 421 | Loss: 0.32680 | Div scale: 1.000\n",
      "NLL: 0.03459 | KL: 0.28603\n",
      "Step: 422 | Loss: 0.33306 | Div scale: 1.000\n",
      "NLL: 0.03447 | KL: 0.35495\n",
      "Step: 423 | Loss: 0.33565 | Div scale: 1.000\n",
      "NLL: 0.03526 | KL: 0.32372\n",
      "Step: 424 | Loss: 0.33463 | Div scale: 1.000\n",
      "NLL: 0.03464 | KL: 0.29076\n",
      "Step: 425 | Loss: 0.34700 | Div scale: 1.000\n",
      "NLL: 0.03356 | KL: 0.42478\n",
      "Step: 426 | Loss: 0.34323 | Div scale: 1.000\n",
      "NLL: 0.03398 | KL: 0.27535\n",
      "Step: 427 | Loss: 0.34728 | Div scale: 1.000\n",
      "NLL: 0.03333 | KL: 0.35037\n",
      "Step: 428 | Loss: 0.34975 | Div scale: 1.000\n",
      "NLL: 0.03438 | KL: 0.33758\n",
      "Step: 429 | Loss: 0.35478 | Div scale: 1.000\n",
      "NLL: 0.03479 | KL: 0.36535\n",
      "Step: 430 | Loss: 0.35452 | Div scale: 1.000\n",
      "NLL: 0.03510 | KL: 0.31701\n",
      "Step: 431 | Loss: 0.36102 | Div scale: 1.000\n",
      "NLL: 0.03306 | KL: 0.38654\n",
      "Step: 432 | Loss: 0.35542 | Div scale: 1.000\n",
      "NLL: 0.03416 | KL: 0.27084\n",
      "Step: 433 | Loss: 0.35711 | Div scale: 1.000\n",
      "NLL: 0.03319 | KL: 0.33911\n",
      "Step: 434 | Loss: 0.35730 | Div scale: 1.000\n",
      "NLL: 0.03286 | KL: 0.32615\n",
      "Step: 435 | Loss: 0.36091 | Div scale: 1.000\n",
      "NLL: 0.03374 | KL: 0.35968\n",
      "Step: 436 | Loss: 0.36007 | Div scale: 1.000\n",
      "NLL: 0.03548 | KL: 0.31701\n",
      "Step: 437 | Loss: 0.35982 | Div scale: 1.000\n",
      "NLL: 0.03533 | KL: 0.32222\n",
      "Step: 438 | Loss: 0.35363 | Div scale: 1.000\n",
      "NLL: 0.03454 | KL: 0.26340\n",
      "Step: 439 | Loss: 0.35261 | Div scale: 1.000\n",
      "NLL: 0.03435 | KL: 0.30908\n",
      "Step: 440 | Loss: 0.35549 | Div scale: 1.000\n",
      "NLL: 0.03531 | KL: 0.34606\n",
      "Step: 441 | Loss: 0.35318 | Div scale: 1.000\n",
      "NLL: 0.03449 | KL: 0.29794\n",
      "Step: 442 | Loss: 0.34668 | Div scale: 1.000\n",
      "NLL: 0.03483 | KL: 0.25338\n",
      "Step: 443 | Loss: 0.34167 | Div scale: 1.000\n",
      "NLL: 0.03454 | KL: 0.26206\n",
      "Step: 444 | Loss: 0.33816 | Div scale: 1.000\n",
      "NLL: 0.03409 | KL: 0.27246\n",
      "Step: 445 | Loss: 0.33467 | Div scale: 1.000\n",
      "NLL: 0.03317 | KL: 0.27007\n",
      "Step: 446 | Loss: 0.32780 | Div scale: 1.000\n",
      "NLL: 0.03407 | KL: 0.23193\n",
      "Step: 447 | Loss: 0.32669 | Div scale: 1.000\n",
      "NLL: 0.03417 | KL: 0.28252\n",
      "Step: 448 | Loss: 0.32307 | Div scale: 1.000\n",
      "NLL: 0.03185 | KL: 0.25866\n",
      "Step: 449 | Loss: 0.31647 | Div scale: 1.000\n",
      "NLL: 0.03510 | KL: 0.22192\n",
      "Step: 450 | Loss: 0.31188 | Div scale: 1.000\n",
      "NLL: 0.03599 | KL: 0.23463\n",
      "Step: 451 | Loss: 0.30862 | Div scale: 1.000\n",
      "NLL: 0.03428 | KL: 0.24500\n",
      "Step: 452 | Loss: 0.30762 | Div scale: 1.000\n",
      "NLL: 0.03475 | KL: 0.26382\n",
      "Step: 453 | Loss: 0.30876 | Div scale: 1.000\n",
      "NLL: 0.03592 | KL: 0.28316\n",
      "Step: 454 | Loss: 0.30674 | Div scale: 1.000\n",
      "NLL: 0.03410 | KL: 0.25439\n",
      "Step: 455 | Loss: 0.30668 | Div scale: 1.000\n",
      "NLL: 0.03564 | KL: 0.27057\n",
      "Step: 456 | Loss: 0.30789 | Div scale: 1.000\n",
      "NLL: 0.03478 | KL: 0.28400\n",
      "Step: 457 | Loss: 0.30634 | Div scale: 1.000\n",
      "NLL: 0.03505 | KL: 0.25730\n",
      "Step: 458 | Loss: 0.30236 | Div scale: 1.000\n",
      "NLL: 0.03431 | KL: 0.23228\n",
      "Step: 459 | Loss: 0.30076 | Div scale: 1.000\n",
      "NLL: 0.03421 | KL: 0.25210\n",
      "Step: 460 | Loss: 0.30180 | Div scale: 1.000\n",
      "NLL: 0.03367 | KL: 0.27747\n",
      "Step: 461 | Loss: 0.30217 | Div scale: 1.000\n",
      "NLL: 0.03287 | KL: 0.27261\n",
      "Step: 462 | Loss: 0.29762 | Div scale: 1.000\n",
      "NLL: 0.03274 | KL: 0.22395\n",
      "Step: 463 | Loss: 0.29553 | Div scale: 1.000\n",
      "NLL: 0.03323 | KL: 0.24350\n",
      "Step: 464 | Loss: 0.29823 | Div scale: 1.000\n",
      "NLL: 0.03366 | KL: 0.28893\n",
      "Step: 465 | Loss: 0.30334 | Div scale: 1.000\n",
      "NLL: 0.03377 | KL: 0.31549\n",
      "Step: 466 | Loss: 0.29926 | Div scale: 1.000\n",
      "NLL: 0.03433 | KL: 0.22825\n",
      "Step: 467 | Loss: 0.30112 | Div scale: 1.000\n",
      "NLL: 0.03397 | KL: 0.28386\n",
      "Step: 468 | Loss: 0.29612 | Div scale: 1.000\n",
      "NLL: 0.03374 | KL: 0.21736\n",
      "Step: 469 | Loss: 0.30085 | Div scale: 1.000\n",
      "NLL: 0.03495 | KL: 0.30854\n",
      "Step: 470 | Loss: 0.30199 | Div scale: 1.000\n",
      "NLL: 0.03351 | KL: 0.27867\n",
      "Step: 471 | Loss: 0.29668 | Div scale: 1.000\n",
      "NLL: 0.03434 | KL: 0.21455\n",
      "Step: 472 | Loss: 0.29773 | Div scale: 1.000\n",
      "NLL: 0.03396 | KL: 0.27326\n",
      "Step: 473 | Loss: 0.28999 | Div scale: 1.000\n",
      "NLL: 0.03379 | KL: 0.18658\n",
      "Step: 474 | Loss: 0.29634 | Div scale: 1.000\n",
      "NLL: 0.03395 | KL: 0.31955\n",
      "Step: 475 | Loss: 0.29015 | Div scale: 1.000\n",
      "NLL: 0.03342 | KL: 0.20102\n",
      "Step: 476 | Loss: 0.29220 | Div scale: 1.000\n",
      "NLL: 0.03512 | KL: 0.27549\n",
      "Step: 477 | Loss: 0.28732 | Div scale: 1.000\n",
      "NLL: 0.03356 | KL: 0.20981\n",
      "Step: 478 | Loss: 0.29515 | Div scale: 1.000\n",
      "NLL: 0.03369 | KL: 0.33196\n",
      "Step: 479 | Loss: 0.29277 | Div scale: 1.000\n",
      "NLL: 0.03247 | KL: 0.23891\n",
      "Step: 480 | Loss: 0.29395 | Div scale: 1.000\n",
      "NLL: 0.03456 | KL: 0.26996\n",
      "Step: 481 | Loss: 0.29878 | Div scale: 1.000\n",
      "NLL: 0.03332 | KL: 0.30899\n",
      "Step: 482 | Loss: 0.29723 | Div scale: 1.000\n",
      "NLL: 0.03324 | KL: 0.25000\n",
      "Step: 483 | Loss: 0.29376 | Div scale: 1.000\n",
      "NLL: 0.03451 | KL: 0.22798\n",
      "Step: 484 | Loss: 0.29453 | Div scale: 1.000\n",
      "NLL: 0.03487 | KL: 0.26663\n",
      "Step: 485 | Loss: 0.29483 | Div scale: 1.000\n",
      "NLL: 0.03247 | KL: 0.26511\n",
      "Step: 486 | Loss: 0.29569 | Div scale: 1.000\n",
      "NLL: 0.03369 | KL: 0.26966\n",
      "Step: 487 | Loss: 0.29653 | Div scale: 1.000\n",
      "NLL: 0.03420 | KL: 0.26994\n",
      "Step: 488 | Loss: 0.30177 | Div scale: 1.000\n",
      "NLL: 0.03510 | KL: 0.31386\n",
      "Step: 489 | Loss: 0.29732 | Div scale: 1.000\n",
      "NLL: 0.03323 | KL: 0.22397\n",
      "Step: 490 | Loss: 0.29435 | Div scale: 1.000\n",
      "NLL: 0.03646 | KL: 0.23115\n",
      "Step: 491 | Loss: 0.29050 | Div scale: 1.000\n",
      "NLL: 0.03339 | KL: 0.22248\n",
      "Step: 492 | Loss: 0.29008 | Div scale: 1.000\n",
      "NLL: 0.03517 | KL: 0.25111\n",
      "Step: 493 | Loss: 0.28706 | Div scale: 1.000\n",
      "NLL: 0.03349 | KL: 0.22645\n",
      "Step: 494 | Loss: 0.28595 | Div scale: 1.000\n",
      "NLL: 0.03478 | KL: 0.24115\n",
      "Step: 495 | Loss: 0.28360 | Div scale: 1.000\n",
      "NLL: 0.03320 | KL: 0.22926\n",
      "Step: 496 | Loss: 0.28602 | Div scale: 1.000\n",
      "NLL: 0.03294 | KL: 0.27482\n",
      "Step: 497 | Loss: 0.28913 | Div scale: 1.000\n",
      "NLL: 0.03476 | KL: 0.28243\n",
      "Step: 498 | Loss: 0.29026 | Div scale: 1.000\n",
      "NLL: 0.03301 | KL: 0.26737\n",
      "Step: 499 | Loss: 0.29221 | Div scale: 1.000\n",
      "NLL: 0.03338 | KL: 0.27635\n",
      "Step: 500 | Loss: 0.28858 | Div scale: 1.000\n",
      "NLL: 0.03406 | KL: 0.22190\n",
      "Step: 501 | Loss: 0.28595 | Div scale: 1.000\n",
      "NLL: 0.03555 | KL: 0.22668\n",
      "Step: 502 | Loss: 0.28148 | Div scale: 1.000\n",
      "NLL: 0.03391 | KL: 0.20738\n",
      "Step: 503 | Loss: 0.28544 | Div scale: 1.000\n",
      "NLL: 0.03430 | KL: 0.28676\n",
      "Step: 504 | Loss: 0.28382 | Div scale: 1.000\n",
      "NLL: 0.03321 | KL: 0.23601\n",
      "Step: 505 | Loss: 0.28274 | Div scale: 1.000\n",
      "NLL: 0.03555 | KL: 0.23751\n",
      "Step: 506 | Loss: 0.28465 | Div scale: 1.000\n",
      "NLL: 0.03274 | KL: 0.26907\n",
      "Step: 507 | Loss: 0.28023 | Div scale: 1.000\n",
      "NLL: 0.03403 | KL: 0.20640\n",
      "Step: 508 | Loss: 0.28134 | Div scale: 1.000\n",
      "NLL: 0.03387 | KL: 0.25749\n",
      "Step: 509 | Loss: 0.27757 | Div scale: 1.000\n",
      "NLL: 0.03312 | KL: 0.21047\n",
      "Step: 510 | Loss: 0.27385 | Div scale: 1.000\n",
      "NLL: 0.03386 | KL: 0.20657\n",
      "Step: 511 | Loss: 0.26958 | Div scale: 1.000\n",
      "NLL: 0.03420 | KL: 0.19697\n",
      "Step: 512 | Loss: 0.26524 | Div scale: 1.000\n",
      "NLL: 0.03452 | KL: 0.19164\n",
      "Step: 513 | Loss: 0.26818 | Div scale: 1.000\n",
      "NLL: 0.03478 | KL: 0.25989\n",
      "Step: 514 | Loss: 0.26409 | Div scale: 1.000\n",
      "NLL: 0.03506 | KL: 0.19222\n",
      "Step: 515 | Loss: 0.26527 | Div scale: 1.000\n",
      "NLL: 0.03413 | KL: 0.24177\n",
      "Step: 516 | Loss: 0.26641 | Div scale: 1.000\n",
      "NLL: 0.03307 | KL: 0.24360\n",
      "Step: 517 | Loss: 0.26790 | Div scale: 1.000\n",
      "NLL: 0.03455 | KL: 0.24675\n",
      "Step: 518 | Loss: 0.26670 | Div scale: 1.000\n",
      "NLL: 0.03413 | KL: 0.22172\n",
      "Step: 519 | Loss: 0.26613 | Div scale: 1.000\n",
      "NLL: 0.03486 | KL: 0.22621\n",
      "Step: 520 | Loss: 0.26668 | Div scale: 1.000\n",
      "NLL: 0.03411 | KL: 0.23749\n",
      "Step: 521 | Loss: 0.26375 | Div scale: 1.000\n",
      "NLL: 0.03502 | KL: 0.20239\n",
      "Step: 522 | Loss: 0.26431 | Div scale: 1.000\n",
      "NLL: 0.03411 | KL: 0.23522\n",
      "Step: 523 | Loss: 0.26909 | Div scale: 1.000\n",
      "NLL: 0.03506 | KL: 0.27704\n",
      "Step: 524 | Loss: 0.26766 | Div scale: 1.000\n",
      "NLL: 0.03555 | KL: 0.21929\n",
      "Step: 525 | Loss: 0.26497 | Div scale: 1.000\n",
      "NLL: 0.03567 | KL: 0.20510\n",
      "Step: 526 | Loss: 0.26395 | Div scale: 1.000\n",
      "NLL: 0.03414 | KL: 0.22060\n",
      "Step: 527 | Loss: 0.26130 | Div scale: 1.000\n",
      "NLL: 0.03367 | KL: 0.20372\n",
      "Step: 528 | Loss: 0.26207 | Div scale: 1.000\n",
      "NLL: 0.03442 | KL: 0.23459\n",
      "Step: 529 | Loss: 0.25950 | Div scale: 1.000\n",
      "NLL: 0.03456 | KL: 0.20183\n",
      "Step: 530 | Loss: 0.26151 | Div scale: 1.000\n",
      "NLL: 0.03387 | KL: 0.24577\n",
      "Step: 531 | Loss: 0.26536 | Div scale: 1.000\n",
      "NLL: 0.03423 | KL: 0.26575\n",
      "Step: 532 | Loss: 0.26321 | Div scale: 1.000\n",
      "NLL: 0.03373 | KL: 0.21011\n",
      "Step: 533 | Loss: 0.26528 | Div scale: 1.000\n",
      "NLL: 0.03371 | KL: 0.25020\n",
      "Step: 534 | Loss: 0.26124 | Div scale: 1.000\n",
      "NLL: 0.03343 | KL: 0.19143\n",
      "Step: 535 | Loss: 0.26755 | Div scale: 1.000\n",
      "NLL: 0.03298 | KL: 0.29136\n",
      "Step: 536 | Loss: 0.26874 | Div scale: 1.000\n",
      "NLL: 0.03408 | KL: 0.24542\n",
      "Step: 537 | Loss: 0.27141 | Div scale: 1.000\n",
      "NLL: 0.03305 | KL: 0.26238\n",
      "Step: 538 | Loss: 0.27489 | Div scale: 1.000\n",
      "NLL: 0.03457 | KL: 0.27161\n",
      "Step: 539 | Loss: 0.28104 | Div scale: 1.000\n",
      "NLL: 0.03426 | KL: 0.30216\n",
      "Step: 540 | Loss: 0.28456 | Div scale: 1.000\n",
      "NLL: 0.03323 | KL: 0.28296\n",
      "Step: 541 | Loss: 0.27954 | Div scale: 1.000\n",
      "NLL: 0.03414 | KL: 0.20029\n",
      "Step: 542 | Loss: 0.27623 | Div scale: 1.000\n",
      "NLL: 0.03379 | KL: 0.21258\n",
      "Step: 543 | Loss: 0.27426 | Div scale: 1.000\n",
      "NLL: 0.03370 | KL: 0.22285\n",
      "Step: 544 | Loss: 0.27466 | Div scale: 1.000\n",
      "NLL: 0.03531 | KL: 0.24292\n",
      "Step: 545 | Loss: 0.27234 | Div scale: 1.000\n",
      "NLL: 0.03286 | KL: 0.21869\n",
      "Step: 546 | Loss: 0.27861 | Div scale: 1.000\n",
      "NLL: 0.03459 | KL: 0.30040\n",
      "Step: 547 | Loss: 0.27604 | Div scale: 1.000\n",
      "NLL: 0.03369 | KL: 0.21921\n",
      "Step: 548 | Loss: 0.27858 | Div scale: 1.000\n",
      "NLL: 0.03523 | KL: 0.26626\n",
      "Step: 549 | Loss: 0.27634 | Div scale: 1.000\n",
      "NLL: 0.03415 | KL: 0.22203\n",
      "Step: 550 | Loss: 0.27420 | Div scale: 1.000\n",
      "NLL: 0.03408 | KL: 0.22088\n",
      "Step: 551 | Loss: 0.27203 | Div scale: 1.000\n",
      "NLL: 0.03286 | KL: 0.21958\n",
      "Step: 552 | Loss: 0.27115 | Div scale: 1.000\n",
      "NLL: 0.03476 | KL: 0.22850\n",
      "Step: 553 | Loss: 0.26690 | Div scale: 1.000\n",
      "NLL: 0.03465 | KL: 0.19400\n",
      "Step: 554 | Loss: 0.26270 | Div scale: 1.000\n",
      "NLL: 0.03487 | KL: 0.19001\n",
      "Step: 555 | Loss: 0.26657 | Div scale: 1.000\n",
      "NLL: 0.03323 | KL: 0.26818\n",
      "Step: 556 | Loss: 0.26525 | Div scale: 1.000\n",
      "NLL: 0.03312 | KL: 0.22028\n",
      "Step: 557 | Loss: 0.26702 | Div scale: 1.000\n",
      "NLL: 0.03399 | KL: 0.24891\n",
      "Step: 558 | Loss: 0.26468 | Div scale: 1.000\n",
      "NLL: 0.03357 | KL: 0.21007\n",
      "Step: 559 | Loss: 0.26250 | Div scale: 1.000\n",
      "NLL: 0.03258 | KL: 0.21035\n",
      "Step: 560 | Loss: 0.26703 | Div scale: 1.000\n",
      "NLL: 0.03458 | KL: 0.27322\n",
      "Step: 561 | Loss: 0.26422 | Div scale: 1.000\n",
      "NLL: 0.03501 | KL: 0.20385\n",
      "Step: 562 | Loss: 0.26209 | Div scale: 1.000\n",
      "NLL: 0.03355 | KL: 0.20938\n",
      "Step: 563 | Loss: 0.25989 | Div scale: 1.000\n",
      "NLL: 0.03294 | KL: 0.20715\n",
      "Step: 564 | Loss: 0.25873 | Div scale: 1.000\n",
      "NLL: 0.03273 | KL: 0.21561\n",
      "Step: 565 | Loss: 0.26175 | Div scale: 1.000\n",
      "NLL: 0.03441 | KL: 0.25447\n",
      "Step: 566 | Loss: 0.26079 | Div scale: 1.000\n",
      "NLL: 0.03454 | KL: 0.21766\n",
      "Step: 567 | Loss: 0.26087 | Div scale: 1.000\n",
      "NLL: 0.03440 | KL: 0.22712\n",
      "Step: 568 | Loss: 0.26179 | Div scale: 1.000\n",
      "NLL: 0.03429 | KL: 0.23578\n",
      "Step: 569 | Loss: 0.25415 | Div scale: 1.000\n",
      "NLL: 0.03462 | KL: 0.15078\n",
      "Step: 570 | Loss: 0.25254 | Div scale: 1.000\n",
      "NLL: 0.03548 | KL: 0.20264\n",
      "Step: 571 | Loss: 0.24926 | Div scale: 1.000\n",
      "NLL: 0.03298 | KL: 0.18675\n",
      "Step: 572 | Loss: 0.25095 | Div scale: 1.000\n",
      "NLL: 0.03487 | KL: 0.23122\n",
      "Step: 573 | Loss: 0.25238 | Div scale: 1.000\n",
      "NLL: 0.03449 | KL: 0.23078\n",
      "Step: 574 | Loss: 0.25050 | Div scale: 1.000\n",
      "NLL: 0.03270 | KL: 0.20088\n",
      "Step: 575 | Loss: 0.24891 | Div scale: 1.000\n",
      "NLL: 0.03357 | KL: 0.20109\n",
      "Step: 576 | Loss: 0.24774 | Div scale: 1.000\n",
      "NLL: 0.03411 | KL: 0.20307\n",
      "Step: 577 | Loss: 0.24246 | Div scale: 1.000\n",
      "NLL: 0.03471 | KL: 0.16027\n",
      "Step: 578 | Loss: 0.24376 | Div scale: 1.000\n",
      "NLL: 0.03410 | KL: 0.22128\n",
      "Step: 579 | Loss: 0.23953 | Div scale: 1.000\n",
      "NLL: 0.03378 | KL: 0.16771\n",
      "Step: 580 | Loss: 0.24126 | Div scale: 1.000\n",
      "NLL: 0.03470 | KL: 0.22212\n",
      "Step: 581 | Loss: 0.23904 | Div scale: 1.000\n",
      "NLL: 0.03380 | KL: 0.18530\n",
      "Step: 582 | Loss: 0.23819 | Div scale: 1.000\n",
      "NLL: 0.03448 | KL: 0.19607\n",
      "Step: 583 | Loss: 0.23659 | Div scale: 1.000\n",
      "NLL: 0.03355 | KL: 0.18861\n",
      "Step: 584 | Loss: 0.23361 | Div scale: 1.000\n",
      "NLL: 0.03353 | KL: 0.17323\n",
      "Step: 585 | Loss: 0.23267 | Div scale: 1.000\n",
      "NLL: 0.03272 | KL: 0.19156\n",
      "Step: 586 | Loss: 0.23299 | Div scale: 1.000\n",
      "NLL: 0.03334 | KL: 0.20247\n",
      "Step: 587 | Loss: 0.23478 | Div scale: 1.000\n",
      "NLL: 0.03355 | KL: 0.21735\n",
      "Step: 588 | Loss: 0.23990 | Div scale: 1.000\n",
      "NLL: 0.03476 | KL: 0.25124\n",
      "Step: 589 | Loss: 0.24094 | Div scale: 1.000\n",
      "NLL: 0.03308 | KL: 0.21721\n",
      "Step: 590 | Loss: 0.25229 | Div scale: 1.000\n",
      "NLL: 0.03437 | KL: 0.32008\n",
      "Step: 591 | Loss: 0.25171 | Div scale: 1.000\n",
      "NLL: 0.03474 | KL: 0.21173\n",
      "Step: 592 | Loss: 0.25326 | Div scale: 1.000\n",
      "NLL: 0.03424 | KL: 0.23302\n",
      "Step: 593 | Loss: 0.25668 | Div scale: 1.000\n",
      "NLL: 0.03477 | KL: 0.25269\n",
      "Step: 594 | Loss: 0.26089 | Div scale: 1.000\n",
      "NLL: 0.03463 | KL: 0.26410\n",
      "Step: 595 | Loss: 0.26253 | Div scale: 1.000\n",
      "NLL: 0.03357 | KL: 0.24378\n",
      "Step: 596 | Loss: 0.27125 | Div scale: 1.000\n",
      "NLL: 0.03471 | KL: 0.31499\n",
      "Step: 597 | Loss: 0.27963 | Div scale: 1.000\n",
      "NLL: 0.03335 | KL: 0.32170\n",
      "Step: 598 | Loss: 0.28241 | Div scale: 1.000\n",
      "NLL: 0.03359 | KL: 0.27379\n",
      "Step: 599 | Loss: 0.29407 | Div scale: 1.000\n",
      "NLL: 0.03318 | KL: 0.36590\n",
      "Step: 600 | Loss: 0.28808 | Div scale: 1.000\n",
      "NLL: 0.03330 | KL: 0.20079\n",
      "Step: 601 | Loss: 0.29216 | Div scale: 1.000\n",
      "NLL: 0.03330 | KL: 0.29562\n",
      "Step: 602 | Loss: 0.29171 | Div scale: 1.000\n",
      "NLL: 0.03286 | KL: 0.25477\n",
      "Step: 603 | Loss: 0.29287 | Div scale: 1.000\n",
      "NLL: 0.03504 | KL: 0.26826\n",
      "Step: 604 | Loss: 0.28919 | Div scale: 1.000\n",
      "NLL: 0.03400 | KL: 0.22211\n",
      "Step: 605 | Loss: 0.28520 | Div scale: 1.000\n",
      "NLL: 0.03319 | KL: 0.21609\n",
      "Step: 606 | Loss: 0.27990 | Div scale: 1.000\n",
      "NLL: 0.03492 | KL: 0.19732\n",
      "Step: 607 | Loss: 0.27586 | Div scale: 1.000\n",
      "NLL: 0.03501 | KL: 0.20445\n",
      "Step: 608 | Loss: 0.27029 | Div scale: 1.000\n",
      "NLL: 0.03309 | KL: 0.18710\n",
      "Step: 609 | Loss: 0.26158 | Div scale: 1.000\n",
      "NLL: 0.03494 | KL: 0.14818\n",
      "Step: 610 | Loss: 0.25450 | Div scale: 1.000\n",
      "NLL: 0.03322 | KL: 0.15763\n",
      "Step: 611 | Loss: 0.25091 | Div scale: 1.000\n",
      "NLL: 0.03347 | KL: 0.18514\n",
      "Step: 612 | Loss: 0.25090 | Div scale: 1.000\n",
      "NLL: 0.03357 | KL: 0.21718\n",
      "Step: 613 | Loss: 0.24807 | Div scale: 1.000\n",
      "NLL: 0.03576 | KL: 0.18691\n",
      "Step: 614 | Loss: 0.24490 | Div scale: 1.000\n",
      "NLL: 0.03550 | KL: 0.18084\n",
      "Step: 615 | Loss: 0.24295 | Div scale: 1.000\n",
      "NLL: 0.03369 | KL: 0.19173\n",
      "Step: 616 | Loss: 0.23930 | Div scale: 1.000\n",
      "NLL: 0.03438 | KL: 0.17207\n",
      "Step: 617 | Loss: 0.23606 | Div scale: 1.000\n",
      "NLL: 0.03403 | KL: 0.17290\n",
      "Step: 618 | Loss: 0.23198 | Div scale: 1.000\n",
      "NLL: 0.03383 | KL: 0.16140\n",
      "Step: 619 | Loss: 0.23073 | Div scale: 1.000\n",
      "NLL: 0.03513 | KL: 0.18436\n",
      "Step: 620 | Loss: 0.22982 | Div scale: 1.000\n",
      "NLL: 0.03531 | KL: 0.18632\n",
      "Step: 621 | Loss: 0.22763 | Div scale: 1.000\n",
      "NLL: 0.03307 | KL: 0.17481\n",
      "Step: 622 | Loss: 0.22824 | Div scale: 1.000\n",
      "NLL: 0.03252 | KL: 0.20127\n",
      "Step: 623 | Loss: 0.22748 | Div scale: 1.000\n",
      "NLL: 0.03507 | KL: 0.18552\n",
      "Step: 624 | Loss: 0.22878 | Div scale: 1.000\n",
      "NLL: 0.03374 | KL: 0.20678\n",
      "Step: 625 | Loss: 0.22449 | Div scale: 1.000\n",
      "NLL: 0.03345 | KL: 0.15237\n",
      "Step: 626 | Loss: 0.22415 | Div scale: 1.000\n",
      "NLL: 0.03395 | KL: 0.18720\n",
      "Step: 627 | Loss: 0.22098 | Div scale: 1.000\n",
      "NLL: 0.03313 | KL: 0.15925\n",
      "Step: 628 | Loss: 0.22291 | Div scale: 1.000\n",
      "NLL: 0.03520 | KL: 0.20510\n",
      "Step: 629 | Loss: 0.22507 | Div scale: 1.000\n",
      "NLL: 0.03420 | KL: 0.21028\n",
      "Step: 630 | Loss: 0.22415 | Div scale: 1.000\n",
      "NLL: 0.03229 | KL: 0.18365\n",
      "Step: 631 | Loss: 0.23177 | Div scale: 1.000\n",
      "NLL: 0.03471 | KL: 0.26558\n",
      "Step: 632 | Loss: 0.23075 | Div scale: 1.000\n",
      "NLL: 0.03444 | KL: 0.18720\n",
      "Step: 633 | Loss: 0.23885 | Div scale: 1.000\n",
      "NLL: 0.03363 | KL: 0.27815\n",
      "Step: 634 | Loss: 0.23866 | Div scale: 1.000\n",
      "NLL: 0.03415 | KL: 0.20271\n",
      "Step: 635 | Loss: 0.23739 | Div scale: 1.000\n",
      "NLL: 0.03353 | KL: 0.19242\n",
      "Step: 636 | Loss: 0.23714 | Div scale: 1.000\n",
      "NLL: 0.03578 | KL: 0.19913\n",
      "Step: 637 | Loss: 0.24013 | Div scale: 1.000\n",
      "NLL: 0.03389 | KL: 0.23320\n",
      "Step: 638 | Loss: 0.24848 | Div scale: 1.000\n",
      "NLL: 0.03372 | KL: 0.28993\n",
      "Step: 639 | Loss: 0.24587 | Div scale: 1.000\n",
      "NLL: 0.03453 | KL: 0.18779\n",
      "Step: 640 | Loss: 0.25254 | Div scale: 1.000\n",
      "NLL: 0.03521 | KL: 0.27737\n",
      "Step: 641 | Loss: 0.25141 | Div scale: 1.000\n",
      "NLL: 0.03350 | KL: 0.20778\n",
      "Step: 642 | Loss: 0.25392 | Div scale: 1.000\n",
      "NLL: 0.03504 | KL: 0.24149\n",
      "Step: 643 | Loss: 0.24993 | Div scale: 1.000\n",
      "NLL: 0.03313 | KL: 0.18083\n",
      "Step: 644 | Loss: 0.24744 | Div scale: 1.000\n",
      "NLL: 0.03657 | KL: 0.18851\n",
      "Step: 645 | Loss: 0.24377 | Div scale: 1.000\n",
      "NLL: 0.03472 | KL: 0.17595\n",
      "Step: 646 | Loss: 0.23918 | Div scale: 1.000\n",
      "NLL: 0.03313 | KL: 0.16481\n",
      "Step: 647 | Loss: 0.23676 | Div scale: 1.000\n",
      "NLL: 0.03271 | KL: 0.18223\n",
      "Step: 648 | Loss: 0.23335 | Div scale: 1.000\n",
      "NLL: 0.03491 | KL: 0.16776\n",
      "Step: 649 | Loss: 0.23155 | Div scale: 1.000\n",
      "NLL: 0.03366 | KL: 0.18170\n",
      "Step: 650 | Loss: 0.23066 | Div scale: 1.000\n",
      "NLL: 0.03221 | KL: 0.19040\n",
      "Step: 651 | Loss: 0.22546 | Div scale: 1.000\n",
      "NLL: 0.03441 | KL: 0.14430\n",
      "Step: 652 | Loss: 0.22613 | Div scale: 1.000\n",
      "NLL: 0.03426 | KL: 0.19788\n",
      "Step: 653 | Loss: 0.22328 | Div scale: 1.000\n",
      "NLL: 0.03367 | KL: 0.16393\n",
      "Step: 654 | Loss: 0.22064 | Div scale: 1.000\n",
      "NLL: 0.03479 | KL: 0.16212\n",
      "Step: 655 | Loss: 0.21969 | Div scale: 1.000\n",
      "NLL: 0.03281 | KL: 0.17828\n",
      "Step: 656 | Loss: 0.21725 | Div scale: 1.000\n",
      "NLL: 0.03427 | KL: 0.16107\n",
      "Step: 657 | Loss: 0.21399 | Div scale: 1.000\n",
      "NLL: 0.03491 | KL: 0.14974\n",
      "Step: 658 | Loss: 0.21460 | Div scale: 1.000\n",
      "NLL: 0.03378 | KL: 0.18630\n",
      "Step: 659 | Loss: 0.21832 | Div scale: 1.000\n",
      "NLL: 0.03564 | KL: 0.21619\n",
      "Step: 660 | Loss: 0.21636 | Div scale: 1.000\n",
      "NLL: 0.03424 | KL: 0.16443\n",
      "Step: 661 | Loss: 0.21998 | Div scale: 1.000\n",
      "NLL: 0.03509 | KL: 0.21744\n",
      "Step: 662 | Loss: 0.22284 | Div scale: 1.000\n",
      "NLL: 0.03388 | KL: 0.21477\n",
      "Step: 663 | Loss: 0.21940 | Div scale: 1.000\n",
      "NLL: 0.03335 | KL: 0.15506\n",
      "Step: 664 | Loss: 0.22301 | Div scale: 1.000\n",
      "NLL: 0.03401 | KL: 0.22150\n",
      "Step: 665 | Loss: 0.22196 | Div scale: 1.000\n",
      "NLL: 0.03370 | KL: 0.17881\n",
      "Step: 666 | Loss: 0.22569 | Div scale: 1.000\n",
      "NLL: 0.03335 | KL: 0.22594\n",
      "Step: 667 | Loss: 0.22131 | Div scale: 1.000\n",
      "NLL: 0.03331 | KL: 0.14857\n",
      "Step: 668 | Loss: 0.21893 | Div scale: 1.000\n",
      "NLL: 0.03468 | KL: 0.16283\n",
      "Step: 669 | Loss: 0.21420 | Div scale: 1.000\n",
      "NLL: 0.03462 | KL: 0.13700\n",
      "Step: 670 | Loss: 0.21317 | Div scale: 1.000\n",
      "NLL: 0.03280 | KL: 0.17108\n",
      "Step: 671 | Loss: 0.21360 | Div scale: 1.000\n",
      "NLL: 0.03388 | KL: 0.18365\n",
      "Step: 672 | Loss: 0.21051 | Div scale: 1.000\n",
      "NLL: 0.03297 | KL: 0.14965\n",
      "Step: 673 | Loss: 0.20894 | Div scale: 1.000\n",
      "NLL: 0.03601 | KL: 0.15879\n",
      "Step: 674 | Loss: 0.20599 | Div scale: 1.000\n",
      "NLL: 0.03320 | KL: 0.14627\n",
      "Step: 675 | Loss: 0.20602 | Div scale: 1.000\n",
      "NLL: 0.03261 | KL: 0.17366\n",
      "Step: 676 | Loss: 0.20629 | Div scale: 1.000\n",
      "NLL: 0.03377 | KL: 0.17498\n",
      "Step: 677 | Loss: 0.20451 | Div scale: 1.000\n",
      "NLL: 0.03373 | KL: 0.15472\n",
      "Step: 678 | Loss: 0.20477 | Div scale: 1.000\n",
      "NLL: 0.03374 | KL: 0.17338\n",
      "Step: 679 | Loss: 0.20275 | Div scale: 1.000\n",
      "NLL: 0.03356 | KL: 0.15097\n",
      "Step: 680 | Loss: 0.20454 | Div scale: 1.000\n",
      "NLL: 0.03308 | KL: 0.18766\n",
      "Step: 681 | Loss: 0.20329 | Div scale: 1.000\n",
      "NLL: 0.03505 | KL: 0.15699\n",
      "Step: 682 | Loss: 0.20259 | Div scale: 1.000\n",
      "NLL: 0.03328 | KL: 0.16296\n",
      "Step: 683 | Loss: 0.20668 | Div scale: 1.000\n",
      "NLL: 0.03430 | KL: 0.20925\n",
      "Step: 684 | Loss: 0.20683 | Div scale: 1.000\n",
      "NLL: 0.03246 | KL: 0.17569\n",
      "Step: 685 | Loss: 0.20919 | Div scale: 1.000\n",
      "NLL: 0.03297 | KL: 0.19748\n",
      "Step: 686 | Loss: 0.20951 | Div scale: 1.000\n",
      "NLL: 0.03450 | KL: 0.17789\n",
      "Step: 687 | Loss: 0.20953 | Div scale: 1.000\n",
      "NLL: 0.03479 | KL: 0.17490\n",
      "Step: 688 | Loss: 0.21295 | Div scale: 1.000\n",
      "NLL: 0.03302 | KL: 0.21073\n",
      "Step: 689 | Loss: 0.20902 | Div scale: 1.000\n",
      "NLL: 0.03465 | KL: 0.13893\n",
      "Step: 690 | Loss: 0.21307 | Div scale: 1.000\n",
      "NLL: 0.03327 | KL: 0.21625\n",
      "Step: 691 | Loss: 0.21133 | Div scale: 1.000\n",
      "NLL: 0.03256 | KL: 0.16314\n",
      "Step: 692 | Loss: 0.20781 | Div scale: 1.000\n",
      "NLL: 0.03450 | KL: 0.14163\n",
      "Step: 693 | Loss: 0.20922 | Div scale: 1.000\n",
      "NLL: 0.03533 | KL: 0.18658\n",
      "Step: 694 | Loss: 0.20910 | Div scale: 1.000\n",
      "NLL: 0.03343 | KL: 0.17457\n",
      "Step: 695 | Loss: 0.20907 | Div scale: 1.000\n",
      "NLL: 0.03399 | KL: 0.17487\n",
      "Step: 696 | Loss: 0.20818 | Div scale: 1.000\n",
      "NLL: 0.03326 | KL: 0.16690\n",
      "Step: 697 | Loss: 0.20803 | Div scale: 1.000\n",
      "NLL: 0.03186 | KL: 0.17478\n",
      "Step: 698 | Loss: 0.20933 | Div scale: 1.000\n",
      "NLL: 0.03279 | KL: 0.18821\n",
      "Step: 699 | Loss: 0.20612 | Div scale: 1.000\n",
      "NLL: 0.03414 | KL: 0.14316\n",
      "Step: 700 | Loss: 0.20919 | Div scale: 1.000\n",
      "NLL: 0.03478 | KL: 0.20201\n",
      "Step: 701 | Loss: 0.20967 | Div scale: 1.000\n",
      "NLL: 0.03392 | KL: 0.18003\n",
      "Step: 702 | Loss: 0.20867 | Div scale: 1.000\n",
      "NLL: 0.03448 | KL: 0.16522\n",
      "Step: 703 | Loss: 0.20978 | Div scale: 1.000\n",
      "NLL: 0.03283 | KL: 0.18692\n",
      "Step: 704 | Loss: 0.20982 | Div scale: 1.000\n",
      "NLL: 0.03473 | KL: 0.17550\n",
      "Step: 705 | Loss: 0.20881 | Div scale: 1.000\n",
      "NLL: 0.03331 | KL: 0.16643\n",
      "Step: 706 | Loss: 0.20709 | Div scale: 1.000\n",
      "NLL: 0.03413 | KL: 0.15746\n",
      "Step: 707 | Loss: 0.20528 | Div scale: 1.000\n",
      "NLL: 0.03467 | KL: 0.15430\n",
      "Step: 708 | Loss: 0.20322 | Div scale: 1.000\n",
      "NLL: 0.03570 | KL: 0.14895\n",
      "Step: 709 | Loss: 0.20083 | Div scale: 1.000\n",
      "NLL: 0.03393 | KL: 0.14543\n",
      "Step: 710 | Loss: 0.20291 | Div scale: 1.000\n",
      "NLL: 0.03274 | KL: 0.18890\n",
      "Step: 711 | Loss: 0.20198 | Div scale: 1.000\n",
      "NLL: 0.03243 | KL: 0.16119\n",
      "Step: 712 | Loss: 0.19898 | Div scale: 1.000\n",
      "NLL: 0.03464 | KL: 0.13731\n",
      "Step: 713 | Loss: 0.19565 | Div scale: 1.000\n",
      "NLL: 0.03467 | KL: 0.13104\n",
      "Step: 714 | Loss: 0.19337 | Div scale: 1.000\n",
      "NLL: 0.03384 | KL: 0.13901\n",
      "Step: 715 | Loss: 0.19246 | Div scale: 1.000\n",
      "NLL: 0.03314 | KL: 0.15107\n",
      "Step: 716 | Loss: 0.19248 | Div scale: 1.000\n",
      "NLL: 0.03301 | KL: 0.15974\n",
      "Step: 717 | Loss: 0.19194 | Div scale: 1.000\n",
      "NLL: 0.03497 | KL: 0.15211\n",
      "Step: 718 | Loss: 0.19146 | Div scale: 1.000\n",
      "NLL: 0.03367 | KL: 0.15341\n",
      "Step: 719 | Loss: 0.18868 | Div scale: 1.000\n",
      "NLL: 0.03387 | KL: 0.12984\n",
      "Step: 720 | Loss: 0.18798 | Div scale: 1.000\n",
      "NLL: 0.03415 | KL: 0.14751\n",
      "Step: 721 | Loss: 0.19024 | Div scale: 1.000\n",
      "NLL: 0.03223 | KL: 0.17836\n",
      "Step: 722 | Loss: 0.18852 | Div scale: 1.000\n",
      "NLL: 0.03427 | KL: 0.13873\n",
      "Step: 723 | Loss: 0.18775 | Div scale: 1.000\n",
      "NLL: 0.03353 | KL: 0.14731\n",
      "Step: 724 | Loss: 0.18843 | Div scale: 1.000\n",
      "NLL: 0.03343 | KL: 0.16110\n",
      "Step: 725 | Loss: 0.18766 | Div scale: 1.000\n",
      "NLL: 0.03543 | KL: 0.14530\n",
      "Step: 726 | Loss: 0.18931 | Div scale: 1.000\n",
      "NLL: 0.03247 | KL: 0.17174\n",
      "Step: 727 | Loss: 0.19270 | Div scale: 1.000\n",
      "NLL: 0.03415 | KL: 0.18908\n",
      "Step: 728 | Loss: 0.19212 | Div scale: 1.000\n",
      "NLL: 0.03404 | KL: 0.15285\n",
      "Step: 729 | Loss: 0.19529 | Div scale: 1.000\n",
      "NLL: 0.03380 | KL: 0.19002\n",
      "Step: 730 | Loss: 0.19802 | Div scale: 1.000\n",
      "NLL: 0.03412 | KL: 0.18840\n",
      "Step: 731 | Loss: 0.20111 | Div scale: 1.000\n",
      "NLL: 0.03510 | KL: 0.19386\n",
      "Step: 732 | Loss: 0.19847 | Div scale: 1.000\n",
      "NLL: 0.03365 | KL: 0.14101\n",
      "Step: 733 | Loss: 0.19907 | Div scale: 1.000\n",
      "NLL: 0.03378 | KL: 0.17074\n",
      "Step: 734 | Loss: 0.20347 | Div scale: 1.000\n",
      "NLL: 0.03514 | KL: 0.20793\n",
      "Step: 735 | Loss: 0.20632 | Div scale: 1.000\n",
      "NLL: 0.03387 | KL: 0.19812\n",
      "Step: 736 | Loss: 0.21220 | Div scale: 1.000\n",
      "NLL: 0.03448 | KL: 0.23060\n",
      "Step: 737 | Loss: 0.21556 | Div scale: 1.000\n",
      "NLL: 0.03449 | KL: 0.21136\n",
      "Step: 738 | Loss: 0.21553 | Div scale: 1.000\n",
      "NLL: 0.03395 | KL: 0.18123\n",
      "Step: 739 | Loss: 0.21446 | Div scale: 1.000\n",
      "NLL: 0.03326 | KL: 0.17162\n",
      "Step: 740 | Loss: 0.21108 | Div scale: 1.000\n",
      "NLL: 0.03230 | KL: 0.14836\n",
      "Step: 741 | Loss: 0.21034 | Div scale: 1.000\n",
      "NLL: 0.03322 | KL: 0.17048\n",
      "Step: 742 | Loss: 0.20905 | Div scale: 1.000\n",
      "NLL: 0.03448 | KL: 0.16290\n",
      "Step: 743 | Loss: 0.20575 | Div scale: 1.000\n",
      "NLL: 0.03261 | KL: 0.14345\n",
      "Step: 744 | Loss: 0.20344 | Div scale: 1.000\n",
      "NLL: 0.03546 | KL: 0.14723\n",
      "Step: 745 | Loss: 0.20396 | Div scale: 1.000\n",
      "NLL: 0.03397 | KL: 0.17463\n",
      "Step: 746 | Loss: 0.20314 | Div scale: 1.000\n",
      "NLL: 0.03513 | KL: 0.16060\n",
      "Step: 747 | Loss: 0.20518 | Div scale: 1.000\n",
      "NLL: 0.03464 | KL: 0.18895\n",
      "Step: 748 | Loss: 0.20461 | Div scale: 1.000\n",
      "NLL: 0.03392 | KL: 0.16551\n",
      "Step: 749 | Loss: 0.20229 | Div scale: 1.000\n",
      "NLL: 0.03339 | KL: 0.14807\n",
      "Step: 750 | Loss: 0.19927 | Div scale: 1.000\n",
      "NLL: 0.03288 | KL: 0.13923\n",
      "Step: 751 | Loss: 0.19732 | Div scale: 1.000\n",
      "NLL: 0.03280 | KL: 0.14689\n",
      "Step: 752 | Loss: 0.19403 | Div scale: 1.000\n",
      "NLL: 0.03448 | KL: 0.12997\n",
      "Step: 753 | Loss: 0.19242 | Div scale: 1.000\n",
      "NLL: 0.03496 | KL: 0.14295\n",
      "Step: 754 | Loss: 0.19253 | Div scale: 1.000\n",
      "NLL: 0.03411 | KL: 0.15947\n",
      "Step: 755 | Loss: 0.19287 | Div scale: 1.000\n",
      "NLL: 0.03448 | KL: 0.16144\n",
      "Step: 756 | Loss: 0.19041 | Div scale: 1.000\n",
      "NLL: 0.03391 | KL: 0.13436\n",
      "Step: 757 | Loss: 0.19016 | Div scale: 1.000\n",
      "NLL: 0.03345 | KL: 0.15444\n",
      "Step: 758 | Loss: 0.18824 | Div scale: 1.000\n",
      "NLL: 0.03383 | KL: 0.13715\n",
      "Step: 759 | Loss: 0.19085 | Div scale: 1.000\n",
      "NLL: 0.03469 | KL: 0.17964\n",
      "Step: 760 | Loss: 0.19427 | Div scale: 1.000\n",
      "NLL: 0.03328 | KL: 0.19177\n",
      "Step: 761 | Loss: 0.19741 | Div scale: 1.000\n",
      "NLL: 0.03357 | KL: 0.19214\n",
      "Step: 762 | Loss: 0.19779 | Div scale: 1.000\n",
      "NLL: 0.03425 | KL: 0.16696\n",
      "Step: 763 | Loss: 0.19560 | Div scale: 1.000\n",
      "NLL: 0.03331 | KL: 0.14254\n",
      "Step: 764 | Loss: 0.19412 | Div scale: 1.000\n",
      "NLL: 0.03529 | KL: 0.14556\n",
      "Step: 765 | Loss: 0.18999 | Div scale: 1.000\n",
      "NLL: 0.03413 | KL: 0.11869\n",
      "Step: 766 | Loss: 0.19086 | Div scale: 1.000\n",
      "NLL: 0.03303 | KL: 0.16564\n",
      "Step: 767 | Loss: 0.18881 | Div scale: 1.000\n",
      "NLL: 0.03449 | KL: 0.13587\n",
      "Step: 768 | Loss: 0.18827 | Div scale: 1.000\n",
      "NLL: 0.03453 | KL: 0.14891\n",
      "Step: 769 | Loss: 0.18544 | Div scale: 1.000\n",
      "NLL: 0.03378 | KL: 0.12619\n",
      "Step: 770 | Loss: 0.18481 | Div scale: 1.000\n",
      "NLL: 0.03474 | KL: 0.14441\n",
      "Step: 771 | Loss: 0.18279 | Div scale: 1.000\n",
      "NLL: 0.03442 | KL: 0.13018\n",
      "Step: 772 | Loss: 0.18513 | Div scale: 1.000\n",
      "NLL: 0.03446 | KL: 0.17174\n",
      "Step: 773 | Loss: 0.18323 | Div scale: 1.000\n",
      "NLL: 0.03389 | KL: 0.13220\n",
      "Step: 774 | Loss: 0.18313 | Div scale: 1.000\n",
      "NLL: 0.03475 | KL: 0.14748\n",
      "Step: 775 | Loss: 0.18360 | Div scale: 1.000\n",
      "NLL: 0.03513 | KL: 0.15268\n",
      "Step: 776 | Loss: 0.18457 | Div scale: 1.000\n",
      "NLL: 0.03397 | KL: 0.15940\n",
      "Step: 777 | Loss: 0.18468 | Div scale: 1.000\n",
      "NLL: 0.03300 | KL: 0.15265\n",
      "Step: 778 | Loss: 0.18507 | Div scale: 1.000\n",
      "NLL: 0.03396 | KL: 0.15459\n",
      "Step: 779 | Loss: 0.18762 | Div scale: 1.000\n",
      "NLL: 0.03450 | KL: 0.17603\n",
      "Step: 780 | Loss: 0.18859 | Div scale: 1.000\n",
      "NLL: 0.03529 | KL: 0.16208\n",
      "Step: 781 | Loss: 0.18883 | Div scale: 1.000\n",
      "NLL: 0.03599 | KL: 0.15499\n",
      "Step: 782 | Loss: 0.18815 | Div scale: 1.000\n",
      "NLL: 0.03338 | KL: 0.14865\n",
      "Step: 783 | Loss: 0.19575 | Div scale: 1.000\n",
      "NLL: 0.03321 | KL: 0.23097\n",
      "Step: 784 | Loss: 0.19919 | Div scale: 1.000\n",
      "NLL: 0.03336 | KL: 0.19678\n",
      "Step: 785 | Loss: 0.19967 | Div scale: 1.000\n",
      "NLL: 0.03316 | KL: 0.17079\n",
      "Step: 786 | Loss: 0.20650 | Div scale: 1.000\n",
      "NLL: 0.03329 | KL: 0.23467\n",
      "Step: 787 | Loss: 0.20929 | Div scale: 1.000\n",
      "NLL: 0.03400 | KL: 0.20039\n",
      "Step: 788 | Loss: 0.20916 | Div scale: 1.000\n",
      "NLL: 0.03404 | KL: 0.17402\n",
      "Step: 789 | Loss: 0.21364 | Div scale: 1.000\n",
      "NLL: 0.03380 | KL: 0.22016\n",
      "Step: 790 | Loss: 0.20874 | Div scale: 1.000\n",
      "NLL: 0.03467 | KL: 0.12995\n",
      "Step: 791 | Loss: 0.21743 | Div scale: 1.000\n",
      "NLL: 0.03334 | KL: 0.26231\n",
      "Step: 792 | Loss: 0.21754 | Div scale: 1.000\n",
      "NLL: 0.03394 | KL: 0.18457\n",
      "Step: 793 | Loss: 0.22007 | Div scale: 1.000\n",
      "NLL: 0.03299 | KL: 0.20990\n",
      "Step: 794 | Loss: 0.22675 | Div scale: 1.000\n",
      "NLL: 0.03420 | KL: 0.25264\n",
      "Step: 795 | Loss: 0.21940 | Div scale: 1.000\n",
      "NLL: 0.03351 | KL: 0.11978\n",
      "Step: 796 | Loss: 0.22519 | Div scale: 1.000\n",
      "NLL: 0.03309 | KL: 0.24422\n",
      "Step: 797 | Loss: 0.22311 | Div scale: 1.000\n",
      "NLL: 0.03568 | KL: 0.16869\n",
      "Step: 798 | Loss: 0.22101 | Div scale: 1.000\n",
      "NLL: 0.03545 | KL: 0.16660\n",
      "Step: 799 | Loss: 0.22631 | Div scale: 1.000\n",
      "NLL: 0.03518 | KL: 0.23886\n",
      "Step: 800 | Loss: 0.22227 | Div scale: 1.000\n",
      "NLL: 0.03512 | KL: 0.15081\n",
      "Step: 801 | Loss: 0.22570 | Div scale: 1.000\n",
      "NLL: 0.03353 | KL: 0.22307\n",
      "Step: 802 | Loss: 0.22765 | Div scale: 1.000\n",
      "NLL: 0.03323 | KL: 0.21198\n",
      "Step: 803 | Loss: 0.22328 | Div scale: 1.000\n",
      "NLL: 0.03461 | KL: 0.14933\n",
      "Step: 804 | Loss: 0.22879 | Div scale: 1.000\n",
      "NLL: 0.03469 | KL: 0.24367\n",
      "Step: 805 | Loss: 0.22321 | Div scale: 1.000\n",
      "NLL: 0.03317 | KL: 0.13984\n",
      "Step: 806 | Loss: 0.22495 | Div scale: 1.000\n",
      "NLL: 0.03467 | KL: 0.20591\n",
      "Step: 807 | Loss: 0.22638 | Div scale: 1.000\n",
      "NLL: 0.03403 | KL: 0.20524\n",
      "Step: 808 | Loss: 0.22106 | Div scale: 1.000\n",
      "NLL: 0.03326 | KL: 0.13990\n",
      "Step: 809 | Loss: 0.22169 | Div scale: 1.000\n",
      "NLL: 0.03319 | KL: 0.19419\n",
      "Step: 810 | Loss: 0.21628 | Div scale: 1.000\n",
      "NLL: 0.03558 | KL: 0.13196\n",
      "Step: 811 | Loss: 0.22978 | Div scale: 1.000\n",
      "NLL: 0.03431 | KL: 0.31697\n",
      "Step: 812 | Loss: 0.23267 | Div scale: 1.000\n",
      "NLL: 0.03479 | KL: 0.22391\n",
      "Step: 813 | Loss: 0.22897 | Div scale: 1.000\n",
      "NLL: 0.03329 | KL: 0.16233\n",
      "Step: 814 | Loss: 0.24106 | Div scale: 1.000\n",
      "NLL: 0.03361 | KL: 0.31631\n",
      "Step: 815 | Loss: 0.23253 | Div scale: 1.000\n",
      "NLL: 0.03462 | KL: 0.12110\n",
      "Step: 816 | Loss: 0.23090 | Div scale: 1.000\n",
      "NLL: 0.03322 | KL: 0.18309\n",
      "Step: 817 | Loss: 0.22595 | Div scale: 1.000\n",
      "NLL: 0.03397 | KL: 0.14735\n",
      "Step: 818 | Loss: 0.22356 | Div scale: 1.000\n",
      "NLL: 0.03378 | KL: 0.16826\n",
      "Step: 819 | Loss: 0.23588 | Div scale: 1.000\n",
      "NLL: 0.03461 | KL: 0.31219\n",
      "Step: 820 | Loss: 0.23177 | Div scale: 1.000\n",
      "NLL: 0.03417 | KL: 0.16057\n",
      "Step: 821 | Loss: 0.22675 | Div scale: 1.000\n",
      "NLL: 0.03321 | KL: 0.14841\n",
      "Step: 822 | Loss: 0.22557 | Div scale: 1.000\n",
      "NLL: 0.03462 | KL: 0.18036\n",
      "Step: 823 | Loss: 0.21742 | Div scale: 1.000\n",
      "NLL: 0.03383 | KL: 0.11026\n",
      "Step: 824 | Loss: 0.22182 | Div scale: 1.000\n",
      "NLL: 0.03537 | KL: 0.22605\n",
      "Step: 825 | Loss: 0.21554 | Div scale: 1.000\n",
      "NLL: 0.03439 | KL: 0.12463\n",
      "Step: 826 | Loss: 0.21862 | Div scale: 1.000\n",
      "NLL: 0.03480 | KL: 0.21152\n",
      "Step: 827 | Loss: 0.22185 | Div scale: 1.000\n",
      "NLL: 0.03379 | KL: 0.21713\n",
      "Step: 828 | Loss: 0.21725 | Div scale: 1.000\n",
      "NLL: 0.03508 | KL: 0.14080\n",
      "Step: 829 | Loss: 0.21988 | Div scale: 1.000\n",
      "NLL: 0.03513 | KL: 0.20835\n",
      "Step: 830 | Loss: 0.22172 | Div scale: 1.000\n",
      "NLL: 0.03429 | KL: 0.20401\n",
      "Step: 831 | Loss: 0.21623 | Div scale: 1.000\n",
      "NLL: 0.03348 | KL: 0.13338\n",
      "Step: 832 | Loss: 0.21602 | Div scale: 1.000\n",
      "NLL: 0.03287 | KL: 0.18125\n",
      "Step: 833 | Loss: 0.21068 | Div scale: 1.000\n",
      "NLL: 0.03423 | KL: 0.12836\n",
      "Step: 834 | Loss: 0.20787 | Div scale: 1.000\n",
      "NLL: 0.03464 | KL: 0.14796\n",
      "Step: 835 | Loss: 0.20393 | Div scale: 1.000\n",
      "NLL: 0.03387 | KL: 0.13458\n",
      "Step: 836 | Loss: 0.19988 | Div scale: 1.000\n",
      "NLL: 0.03497 | KL: 0.12848\n",
      "Step: 837 | Loss: 0.20053 | Div scale: 1.000\n",
      "NLL: 0.03261 | KL: 0.17376\n",
      "Step: 838 | Loss: 0.19509 | Div scale: 1.000\n",
      "NLL: 0.03503 | KL: 0.11109\n",
      "Step: 839 | Loss: 0.19441 | Div scale: 1.000\n",
      "NLL: 0.03441 | KL: 0.15390\n",
      "Step: 840 | Loss: 0.19126 | Div scale: 1.000\n",
      "NLL: 0.03436 | KL: 0.12858\n",
      "Step: 841 | Loss: 0.18946 | Div scale: 1.000\n",
      "NLL: 0.03431 | KL: 0.13892\n",
      "Step: 842 | Loss: 0.19116 | Div scale: 1.000\n",
      "NLL: 0.03417 | KL: 0.17223\n",
      "Step: 843 | Loss: 0.19683 | Div scale: 1.000\n",
      "NLL: 0.03368 | KL: 0.21423\n",
      "Step: 844 | Loss: 0.19356 | Div scale: 1.000\n",
      "NLL: 0.03395 | KL: 0.13013\n",
      "Step: 845 | Loss: 0.18982 | Div scale: 1.000\n",
      "NLL: 0.03340 | KL: 0.12277\n",
      "Step: 846 | Loss: 0.18724 | Div scale: 1.000\n",
      "NLL: 0.03439 | KL: 0.12962\n",
      "Step: 847 | Loss: 0.18596 | Div scale: 1.000\n",
      "NLL: 0.03380 | KL: 0.14065\n",
      "Step: 848 | Loss: 0.18592 | Div scale: 1.000\n",
      "NLL: 0.03489 | KL: 0.15064\n",
      "Step: 849 | Loss: 0.18498 | Div scale: 1.000\n",
      "NLL: 0.03287 | KL: 0.14368\n",
      "Step: 850 | Loss: 0.18642 | Div scale: 1.000\n",
      "NLL: 0.03345 | KL: 0.16598\n",
      "Step: 851 | Loss: 0.18600 | Div scale: 1.000\n",
      "NLL: 0.03439 | KL: 0.14782\n",
      "Step: 852 | Loss: 0.18361 | Div scale: 1.000\n",
      "NLL: 0.03305 | KL: 0.12901\n",
      "Step: 853 | Loss: 0.18574 | Div scale: 1.000\n",
      "NLL: 0.03338 | KL: 0.17159\n",
      "Step: 854 | Loss: 0.18550 | Div scale: 1.000\n",
      "NLL: 0.03461 | KL: 0.14872\n",
      "Step: 855 | Loss: 0.18988 | Div scale: 1.000\n",
      "NLL: 0.03367 | KL: 0.19557\n",
      "Step: 856 | Loss: 0.19438 | Div scale: 1.000\n",
      "NLL: 0.03483 | KL: 0.20012\n",
      "Step: 857 | Loss: 0.19390 | Div scale: 1.000\n",
      "NLL: 0.03377 | KL: 0.15580\n",
      "Step: 858 | Loss: 0.19843 | Div scale: 1.000\n",
      "NLL: 0.03415 | KL: 0.20508\n",
      "Step: 859 | Loss: 0.19464 | Div scale: 1.000\n",
      "NLL: 0.03394 | KL: 0.12652\n",
      "Step: 860 | Loss: 0.19574 | Div scale: 1.000\n",
      "NLL: 0.03419 | KL: 0.17144\n",
      "Step: 861 | Loss: 0.19948 | Div scale: 1.000\n",
      "NLL: 0.03378 | KL: 0.19944\n",
      "Step: 862 | Loss: 0.19438 | Div scale: 1.000\n",
      "NLL: 0.03413 | KL: 0.11430\n",
      "Step: 863 | Loss: 0.19866 | Div scale: 1.000\n",
      "NLL: 0.03285 | KL: 0.20437\n",
      "Step: 864 | Loss: 0.20033 | Div scale: 1.000\n",
      "NLL: 0.03471 | KL: 0.18057\n",
      "Step: 865 | Loss: 0.19803 | Div scale: 1.000\n",
      "NLL: 0.03438 | KL: 0.14301\n",
      "Step: 866 | Loss: 0.20259 | Div scale: 1.000\n",
      "NLL: 0.03358 | KL: 0.21005\n",
      "Step: 867 | Loss: 0.20183 | Div scale: 1.000\n",
      "NLL: 0.03453 | KL: 0.16047\n",
      "Step: 868 | Loss: 0.19753 | Div scale: 1.000\n",
      "NLL: 0.03396 | KL: 0.12481\n",
      "Step: 869 | Loss: 0.19959 | Div scale: 1.000\n",
      "NLL: 0.03638 | KL: 0.18180\n",
      "Step: 870 | Loss: 0.19542 | Div scale: 1.000\n",
      "NLL: 0.03263 | KL: 0.12529\n",
      "Step: 871 | Loss: 0.19509 | Div scale: 1.000\n",
      "NLL: 0.03286 | KL: 0.15925\n",
      "Step: 872 | Loss: 0.19442 | Div scale: 1.000\n",
      "NLL: 0.03491 | KL: 0.15341\n",
      "Step: 873 | Loss: 0.19244 | Div scale: 1.000\n",
      "NLL: 0.03548 | KL: 0.13913\n",
      "Step: 874 | Loss: 0.19084 | Div scale: 1.000\n",
      "NLL: 0.03442 | KL: 0.14206\n",
      "Step: 875 | Loss: 0.19452 | Div scale: 1.000\n",
      "NLL: 0.03501 | KL: 0.19266\n",
      "Step: 876 | Loss: 0.19549 | Div scale: 1.000\n",
      "NLL: 0.03419 | KL: 0.16997\n",
      "Step: 877 | Loss: 0.19371 | Div scale: 1.000\n",
      "NLL: 0.03417 | KL: 0.14356\n",
      "Step: 878 | Loss: 0.19428 | Div scale: 1.000\n",
      "NLL: 0.03210 | KL: 0.16733\n",
      "Step: 879 | Loss: 0.18872 | Div scale: 1.000\n",
      "NLL: 0.03349 | KL: 0.10518\n",
      "Step: 880 | Loss: 0.18802 | Div scale: 1.000\n",
      "NLL: 0.03488 | KL: 0.14681\n",
      "Step: 881 | Loss: 0.18857 | Div scale: 1.000\n",
      "NLL: 0.03432 | KL: 0.15920\n",
      "Step: 882 | Loss: 0.18823 | Div scale: 1.000\n",
      "NLL: 0.03456 | KL: 0.15063\n",
      "Step: 883 | Loss: 0.19534 | Div scale: 1.000\n",
      "NLL: 0.03545 | KL: 0.22382\n",
      "Step: 884 | Loss: 0.19413 | Div scale: 1.000\n",
      "NLL: 0.03457 | KL: 0.14868\n",
      "Step: 885 | Loss: 0.19792 | Div scale: 1.000\n",
      "NLL: 0.03564 | KL: 0.19645\n",
      "Step: 886 | Loss: 0.20853 | Div scale: 1.000\n",
      "NLL: 0.03334 | KL: 0.27069\n",
      "Step: 887 | Loss: 0.20583 | Div scale: 1.000\n",
      "NLL: 0.03377 | KL: 0.14771\n",
      "Step: 888 | Loss: 0.20629 | Div scale: 1.000\n",
      "NLL: 0.03357 | KL: 0.17688\n",
      "Step: 889 | Loss: 0.20424 | Div scale: 1.000\n",
      "NLL: 0.03289 | KL: 0.15293\n",
      "Step: 890 | Loss: 0.20253 | Div scale: 1.000\n",
      "NLL: 0.03413 | KL: 0.15297\n",
      "Step: 891 | Loss: 0.21381 | Div scale: 1.000\n",
      "NLL: 0.03494 | KL: 0.28040\n",
      "Step: 892 | Loss: 0.21016 | Div scale: 1.000\n",
      "NLL: 0.03439 | KL: 0.14294\n",
      "Step: 893 | Loss: 0.21299 | Div scale: 1.000\n",
      "NLL: 0.03376 | KL: 0.20471\n",
      "Step: 894 | Loss: 0.21545 | Div scale: 1.000\n",
      "NLL: 0.03465 | KL: 0.20288\n",
      "Step: 895 | Loss: 0.20969 | Div scale: 1.000\n",
      "NLL: 0.03469 | KL: 0.12316\n",
      "Step: 896 | Loss: 0.21800 | Div scale: 1.000\n",
      "NLL: 0.03360 | KL: 0.25923\n",
      "Step: 897 | Loss: 0.21685 | Div scale: 1.000\n",
      "NLL: 0.03498 | KL: 0.17148\n",
      "Step: 898 | Loss: 0.21289 | Div scale: 1.000\n",
      "NLL: 0.03417 | KL: 0.14307\n",
      "Step: 899 | Loss: 0.21874 | Div scale: 1.000\n",
      "NLL: 0.03375 | KL: 0.23764\n",
      "Step: 900 | Loss: 0.21518 | Div scale: 1.000\n",
      "NLL: 0.03534 | KL: 0.14783\n",
      "Step: 901 | Loss: 0.21125 | Div scale: 1.000\n",
      "NLL: 0.03371 | KL: 0.14213\n",
      "Step: 902 | Loss: 0.22007 | Div scale: 1.000\n",
      "NLL: 0.03557 | KL: 0.26389\n",
      "Step: 903 | Loss: 0.21618 | Div scale: 1.000\n",
      "NLL: 0.03568 | KL: 0.14550\n",
      "Step: 904 | Loss: 0.21214 | Div scale: 1.000\n",
      "NLL: 0.03447 | KL: 0.14138\n",
      "Step: 905 | Loss: 0.20965 | Div scale: 1.000\n",
      "NLL: 0.03399 | KL: 0.15325\n",
      "Step: 906 | Loss: 0.20280 | Div scale: 1.000\n",
      "NLL: 0.03469 | KL: 0.10647\n",
      "Step: 907 | Loss: 0.20154 | Div scale: 1.000\n",
      "NLL: 0.03417 | KL: 0.15602\n",
      "Step: 908 | Loss: 0.19672 | Div scale: 1.000\n",
      "NLL: 0.03344 | KL: 0.11987\n",
      "Step: 909 | Loss: 0.19434 | Div scale: 1.000\n",
      "NLL: 0.03460 | KL: 0.13835\n",
      "Step: 910 | Loss: 0.19462 | Div scale: 1.000\n",
      "NLL: 0.03472 | KL: 0.16241\n",
      "Step: 911 | Loss: 0.19114 | Div scale: 1.000\n",
      "NLL: 0.03312 | KL: 0.12669\n",
      "Step: 912 | Loss: 0.18954 | Div scale: 1.000\n",
      "NLL: 0.03237 | KL: 0.14278\n",
      "Step: 913 | Loss: 0.18947 | Div scale: 1.000\n",
      "NLL: 0.03307 | KL: 0.15579\n",
      "Step: 914 | Loss: 0.18418 | Div scale: 1.000\n",
      "NLL: 0.03402 | KL: 0.10249\n",
      "Step: 915 | Loss: 0.17953 | Div scale: 1.000\n",
      "NLL: 0.03448 | KL: 0.10322\n",
      "Step: 916 | Loss: 0.17819 | Div scale: 1.000\n",
      "NLL: 0.03447 | KL: 0.13163\n",
      "Step: 917 | Loss: 0.17877 | Div scale: 1.000\n",
      "NLL: 0.03640 | KL: 0.14759\n",
      "Step: 918 | Loss: 0.17419 | Div scale: 1.000\n",
      "NLL: 0.03350 | KL: 0.09954\n",
      "Step: 919 | Loss: 0.17143 | Div scale: 1.000\n",
      "NLL: 0.03497 | KL: 0.11157\n",
      "Step: 920 | Loss: 0.16777 | Div scale: 1.000\n",
      "NLL: 0.03369 | KL: 0.10115\n",
      "Step: 921 | Loss: 0.16625 | Div scale: 1.000\n",
      "NLL: 0.03379 | KL: 0.11875\n",
      "Step: 922 | Loss: 0.16430 | Div scale: 1.000\n",
      "NLL: 0.03456 | KL: 0.11220\n",
      "Step: 923 | Loss: 0.16247 | Div scale: 1.000\n",
      "NLL: 0.03517 | KL: 0.11086\n",
      "Step: 924 | Loss: 0.16448 | Div scale: 1.000\n",
      "NLL: 0.03551 | KL: 0.14702\n",
      "Step: 925 | Loss: 0.16690 | Div scale: 1.000\n",
      "NLL: 0.03523 | KL: 0.15350\n",
      "Step: 926 | Loss: 0.16418 | Div scale: 1.000\n",
      "NLL: 0.03296 | KL: 0.10673\n",
      "Step: 927 | Loss: 0.16450 | Div scale: 1.000\n",
      "NLL: 0.03337 | KL: 0.13400\n",
      "Step: 928 | Loss: 0.16806 | Div scale: 1.000\n",
      "NLL: 0.03348 | KL: 0.16660\n",
      "Step: 929 | Loss: 0.16989 | Div scale: 1.000\n",
      "NLL: 0.03480 | KL: 0.15160\n",
      "Step: 930 | Loss: 0.16751 | Div scale: 1.000\n",
      "NLL: 0.03360 | KL: 0.11246\n",
      "Step: 931 | Loss: 0.17091 | Div scale: 1.000\n",
      "NLL: 0.03375 | KL: 0.16775\n",
      "Step: 932 | Loss: 0.16719 | Div scale: 1.000\n",
      "NLL: 0.03524 | KL: 0.09844\n",
      "Step: 933 | Loss: 0.16931 | Div scale: 1.000\n",
      "NLL: 0.03389 | KL: 0.15451\n",
      "Step: 934 | Loss: 0.17157 | Div scale: 1.000\n",
      "NLL: 0.03381 | KL: 0.15814\n",
      "Step: 935 | Loss: 0.17191 | Div scale: 1.000\n",
      "NLL: 0.03261 | KL: 0.14234\n",
      "Step: 936 | Loss: 0.17190 | Div scale: 1.000\n",
      "NLL: 0.03377 | KL: 0.13801\n",
      "Step: 937 | Loss: 0.16913 | Div scale: 1.000\n",
      "NLL: 0.03584 | KL: 0.10844\n"
     ]
    }
   ],
   "source": [
    "train(data=dataloader, vae=vae, optimizer=optimizer, args=train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1202b5-fd5b-4386-bb67-ab169dc031b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
